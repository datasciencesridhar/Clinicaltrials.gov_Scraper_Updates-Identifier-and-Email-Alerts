{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No args supplied at run-time. We will use value supplied in script\n",
      "No args supplied at run-time. We will use value supplied in script\n",
      "trial\n",
      "C:\\Users\\11201312\\Desktop\\output2\\trial\\trial\\\n",
      "C:\\Users\\11201312\\Desktop\\output2\\trial\\trial\n",
      "path already exists - trying a different path name: C:\\Users\\11201312\\Desktop\\output2\\trial\\trial-1026\\\n",
      "queryHeader before: {'Accept': 'application/json', 'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IkI2N0NCRTQ1Q0I5Qjc0QTc0NDZBRDMwQUQxNkZGQjM1NjAzMEE3RDgiLCJ0eXAiOiJhdCtqd3QiLCJ4NXQiOiJ0bnktUmN1YmRLZEVhdE1LMFdfN05XQXdwOWcifQ.eyJuYmYiOjE1ODg4NTk2OTgsImV4cCI6MTU4ODk0NjA5OCwiaXNzIjoiaHR0cHM6Ly9pZGVudGl0eS5waGFybWFpbnRlbGxpZ2VuY2UuaW5mb3JtYS5jb20iLCJhdWQiOiJjdXN0b21lci1hcGkiLCJjbGllbnRfaWQiOiJjdXN0b21lcl9hcGlfY2xpZW50X25hdGl2ZSIsInN1YiI6ImMzMTBmNGRhLWZkNjctNDg0Ni1iMjJhLWMzYTBmZTBkNmQ0ZiIsImF1dGhfdGltZSI6MTU4ODg1OTY5NywiaWRwIjoiU2FsZXNmb3JjZSIsInNmaWQiOiIwMDU0STAwMDAwNWp2eVRRQVEiLCJhY2NvdW50X2xvZ2luIjoic3JpZGhhcmt1bWFyLmthdGthbUB0ZWNod2F2ZS5uZXQiLCJhZmZuYW0iOiJUZWNod2F2ZSIsImNhcGlwIjoiY2FwaWYtdHJpYWwsY2FwaXMtdHJpYWwiLCJjYXBpZi10cmlhbC10eXBlIjoiRnVsbCIsImNhcGlmLXRyaWFsLWlkIjoiYTRkNEkwMDAwMDA5bnFWUUFRIiwiY2FwaXMtdHJpYWwtdHlwZSI6IkZ1bGwiLCJjYXBpcy10cmlhbC1pZCI6ImE0ZDRJMDAwMDAwOW5xVlFBUSIsInNjb3BlIjpbImN1c3RvbWVyLWFwaSJdLCJhbXIiOlsicGFzc3dvcmQiXX0.aGolwACtDyGs9675lBjh-kWCh-wLqK4Xh4IhQgf9npuRCMTLu1YvIBakWjwVmTBJsaC54CftIapsCGo3tfwk5SxviVjZy4Rxib8JCHktnurpPfWERfmV5INFOIy8HRwgunn9eQCL5FEjlQU3WewzcKAlKbkCqLP4tBlGMha6jBcXUXrJbh_RQ171q6pQ17B_876ahfWkl7fo_R0fUzhRhB_sQ2RBUP8UEGbHO-1UtsQym7ReKDqQJ6_DJb_hjF88OF-j9dPTQALa2GUA1Jo7Z0bZ-w6Zm0ikhRnkDlxVqBF96MO9y6dvKL1xiZsVSNtcf1x2qeWtk4IBjR15beVnqQ'}\n",
      "\n",
      "queryHeader after: {'Accept': 'application/json', 'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IkI2N0NCRTQ1Q0I5Qjc0QTc0NDZBRDMwQUQxNkZGQjM1NjAzMEE3RDgiLCJ0eXAiOiJhdCtqd3QiLCJ4NXQiOiJ0bnktUmN1YmRLZEVhdE1LMFdfN05XQXdwOWcifQ.eyJuYmYiOjE1ODg4NTk2OTgsImV4cCI6MTU4ODk0NjA5OCwiaXNzIjoiaHR0cHM6Ly9pZGVudGl0eS5waGFybWFpbnRlbGxpZ2VuY2UuaW5mb3JtYS5jb20iLCJhdWQiOiJjdXN0b21lci1hcGkiLCJjbGllbnRfaWQiOiJjdXN0b21lcl9hcGlfY2xpZW50X25hdGl2ZSIsInN1YiI6ImMzMTBmNGRhLWZkNjctNDg0Ni1iMjJhLWMzYTBmZTBkNmQ0ZiIsImF1dGhfdGltZSI6MTU4ODg1OTY5NywiaWRwIjoiU2FsZXNmb3JjZSIsInNmaWQiOiIwMDU0STAwMDAwNWp2eVRRQVEiLCJhY2NvdW50X2xvZ2luIjoic3JpZGhhcmt1bWFyLmthdGthbUB0ZWNod2F2ZS5uZXQiLCJhZmZuYW0iOiJUZWNod2F2ZSIsImNhcGlwIjoiY2FwaWYtdHJpYWwsY2FwaXMtdHJpYWwiLCJjYXBpZi10cmlhbC10eXBlIjoiRnVsbCIsImNhcGlmLXRyaWFsLWlkIjoiYTRkNEkwMDAwMDA5bnFWUUFRIiwiY2FwaXMtdHJpYWwtdHlwZSI6IkZ1bGwiLCJjYXBpcy10cmlhbC1pZCI6ImE0ZDRJMDAwMDAwOW5xVlFBUSIsInNjb3BlIjpbImN1c3RvbWVyLWFwaSJdLCJhbXIiOlsicGFzc3dvcmQiXX0.aGolwACtDyGs9675lBjh-kWCh-wLqK4Xh4IhQgf9npuRCMTLu1YvIBakWjwVmTBJsaC54CftIapsCGo3tfwk5SxviVjZy4Rxib8JCHktnurpPfWERfmV5INFOIy8HRwgunn9eQCL5FEjlQU3WewzcKAlKbkCqLP4tBlGMha6jBcXUXrJbh_RQ171q6pQ17B_876ahfWkl7fo_R0fUzhRhB_sQ2RBUP8UEGbHO-1UtsQym7ReKDqQJ6_DJb_hjF88OF-j9dPTQALa2GUA1Jo7Z0bZ-w6Zm0ikhRnkDlxVqBF96MO9y6dvKL1xiZsVSNtcf1x2qeWtk4IBjR15beVnqQ', 'Content-Type': 'application/json'}\n",
      "Storing json from ad-hoc query\n",
      "need to update access_token\n",
      "access token getting updated\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(7380 bytes read, 2860 more expected)', IncompleteRead(7380 bytes read, 2860 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    668\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m                 decoded = self._decode(chunk, decode_content=decode_content,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(7380 bytes read, 2860 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    693\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Connection broken: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(7380 bytes read, 2860 more expected)', IncompleteRead(7380 bytes read, 2860 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-043dcf2af0d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1987\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1988\u001b[1;33m     \u001b[0mRA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[1;31m#import schedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-043dcf2af0d3>\u001b[0m in \u001b[0;36mRA\u001b[1;34m()\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0mrunQueries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypeOfRequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchangeDate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathDelimiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-043dcf2af0d3>\u001b[0m in \u001b[0;36mrunQueries\u001b[1;34m(base_path, typeOfRequest, product, payload, changeDate, outputFormat, pathDelimiter)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"queryHeader after: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryHeader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Storing json from ad-hoc query\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                 \u001b[0mgetPaginationURLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueryHeader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproductString\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathDelimiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;34m'change'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypeOfRequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-043dcf2af0d3>\u001b[0m in \u001b[0;36mgetPaginationURLS\u001b[1;34m(product, base_query, queryHeader, path, payload, productString, pathDelimiter)\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;31m#queryHeader.update({'Content-Type':'application/json'})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mqueryHeader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Accept'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'application/json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Authorization'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Content-Type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'application/json'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mnewQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueryHeader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewQuery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \"\"\"\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(7380 bytes read, 2860 more expected)', IncompleteRead(7380 bytes read, 2860 more expected))"
     ]
    }
   ],
   "source": [
    "def RA():\n",
    "    #1\n",
    "    import requests, sys, os, json,  time, csv\n",
    "    from datetime import date, datetime #, timezone\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    import configparser\n",
    "    parser = configparser.ConfigParser()\n",
    "    parser.read('config.ini')\n",
    "    user_email = parser.get('credentials','username')\n",
    "    user_password = parser.get('credentials','password')\n",
    "    access_token = parser.get('credentials','access_token')\n",
    "\n",
    "    def updateAccessToken():\n",
    "        print('access token getting updated')\n",
    "        authHeaders = {'Authorization': 'Basic Y3VzdG9tZXJfYXBpX2NsaWVudF9uYXRpdmU6NTIwNzhjMGItMTI5Mi00MGVhLWFkYjAtOWE4MWY4OGNjZDMy'}\n",
    "        authData = {'grant_type': 'password', 'username': user_email, 'password': user_password, 'scope': 'customer-api'}\n",
    "        r = requests.post('https://identity.pharmaintelligence.informa.com/connect/token', headers=authHeaders, data=authData)\n",
    "        res = r.json()\n",
    "        access_token = '{} {}'.format(res['token_type'], res['access_token'])\n",
    "        parser.set('credentials', 'access_token', access_token)\n",
    "        with open('config.ini', 'w') as configfile:\n",
    "            parser.write(configfile)\n",
    "        return access_token\n",
    "\n",
    "    def read_input(inputListPath):\n",
    "        #Assumes you have a header row in your CSV file\n",
    "        inputList = []\n",
    "        with open(inputListPath, 'r') as csvfile:\n",
    "            numRows = 0\n",
    "            for row in csv.reader(csvfile.read().splitlines()):\n",
    "                if numRows != 0:\n",
    "                    inputList.append(row)\n",
    "                numRows += 1\n",
    "        return inputList\n",
    "\n",
    "    def checkSubEndpoint(product):\n",
    "        newVars = []\n",
    "        subEndpoint = \"\"\n",
    "        apiEndpointMap = read_input('./apiURLEndpointMap.csv')\n",
    "\n",
    "        for item in apiEndpointMap:\n",
    "            if product == item[0]:\n",
    "                product = item[1]\n",
    "                subEndpoint = item[2]\n",
    "\n",
    "        if len(subEndpoint) > 0:\n",
    "            newVars.append(product)\n",
    "            newVars.append(subEndpoint)\n",
    "\n",
    "        return newVars\n",
    "\n",
    "    def getXMLPaginationURLS(product, base_query, queryHeader, path, payload, productString, pathDelimiter):\n",
    "        #run initial query & get XML contents\n",
    "        \n",
    "        \n",
    "        #query = requests.get(base_query, headers=queryHeader)\n",
    "        if payload != \"\":\n",
    "            \n",
    "            queryHeader.update({'Accept':'application/xml'})\n",
    "            query = requests.post(base_query, headers=queryHeader, data=payload)\n",
    "        else:\n",
    "            query = requests.get(base_query, headers=queryHeader)\n",
    "        \n",
    "        \n",
    "        access_token = parser.get('credentials','access_token')\n",
    "        #print(access_token)\n",
    "        if ((query.status_code == 400) or (query.status_code == 401)):\n",
    "            print('need to update access_token')\n",
    "            new_access_token = updateAccessToken()\n",
    "            newQuery = \"\"\n",
    "            if payload != \"\":\n",
    "                #queryHeader.update({'Content-Type':'application/xml'})\n",
    "                queryHeader = {'Accept': 'application/xml','Authorization': new_access_token,'Content-Type': 'application/json'}\n",
    "                newQuery = requests.post(base_query, headers=queryHeader, new_data=payload)\n",
    "                query = newQuery\n",
    "            else:\n",
    "                queryHeader = {'Accept': 'application/xml','Authorization': new_access_token}\n",
    "                newQuery = requests.get(base_query, headers=queryHeader)\n",
    "                query = newQuery\n",
    "            if newQuery.status_code == 200:\n",
    "                access_token = new_access_token\n",
    "            if newQuery.status_code != 200:\n",
    "                print(\"Something not working.\")\n",
    "                return False\n",
    "        #Define First record \n",
    "        productVal =  product\n",
    "        \n",
    "        # Keep the labels for products like drugeventprofile or drugprogrammeaggregationcompanycountry\n",
    "        if productString != \"\":\n",
    "            productVal = productString\n",
    "        \n",
    "        print(path)\n",
    "        \n",
    "        filename = str(path) + str(pathDelimiter) + str(productVal) + str('-export-pg-1.xml')\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        tree = query.text\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(tree.encode('utf-8'))\n",
    "            \n",
    "        \n",
    "        print(\"Processing XML requests. Do not close this window...\")\n",
    "        \n",
    "        cleanTree = tree.encode('UTF-8','ignore')\n",
    "        try:\n",
    "            cleanTreeSubstring = str(cleanTree).split('<NextPage>')\n",
    "            cleanTreeSubstringPt2 = cleanTreeSubstring[1].split('</NextPage>')\n",
    "            nextPage = cleanTreeSubstringPt2[0]\n",
    "        except Exception:\n",
    "            #cleanTreeSubstring = \"\"\n",
    "            #nextPage is None\n",
    "            errorCode = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "        try:\n",
    "        \n",
    "            while nextPage is not None:\n",
    "        \n",
    "                try:\n",
    "                \n",
    "                    if payload != \"\":\n",
    "                        nextQuery = requests.post(nextPage, headers=queryHeader, data=payload)\n",
    "                    else:\n",
    "                        nextQuery = requests.get(nextPage, headers=queryHeader)\n",
    "                    \n",
    "                    # In rare cases, you may encounter a bad response from the API server. \n",
    "                    # When this occurs, the outage is expected to be less than 5 minutes (300 seconds)\n",
    "                    # The error trap below can keep your process alive for 3 attempts\n",
    "                    j = 0\n",
    "                    while nextQuery.status_code == 500:\n",
    "                        print ('internal server error 500 - retrying again in 5 minutes')\n",
    "                        j += 1\n",
    "                        time.sleep(300)\n",
    "                        print ('Retrying now')\n",
    "                        \n",
    "                        if payload != \"\":\n",
    "                            nextQuery = requests.post(nextPage, headers=queryHeader, data=payload)\n",
    "                        else:\n",
    "                            nextQuery = requests.get(nextPage, headers=queryHeader)\n",
    "                        \n",
    "                        #nextQuery = requests.get(nextPage, headers=queryHeader)\n",
    "                        tree = nextQuery.text\n",
    "                        if j == 3:\n",
    "                            print (\"3 unsuccessful attempts. Shutting down now. Please contact PharmaAPIFeedback@informa.com to report an outage\")\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "                    nextTree = nextQuery.text\n",
    "                    \n",
    "                    \n",
    "                    #Define Next record \n",
    "                    filename = str(path) + str(pathDelimiter) + str(productVal) + '-export-pg-'+ str(i+2) +'.xml'\n",
    "                    #time.sleep(1)\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(nextTree.encode('utf-8'))\n",
    "                        \n",
    "            \n",
    "                    nextCleanTree = nextTree.encode('UTF-8','ignore')\n",
    "                    \n",
    "                    try:\n",
    "                    \n",
    "                        nextCleanTreeSubstring = str(nextCleanTree).split('<NextPage>')\n",
    "                        nextCleanTreeSubstringPt2 = nextCleanTreeSubstring[1].split('</NextPage>')\n",
    "                        nextPage = nextCleanTreeSubstringPt2[0]\n",
    "                    except:\n",
    "                        nextPage = None \n",
    "                \n",
    "                except Exception:\n",
    "                    errorCode = 0\n",
    "                \n",
    "                i+= 1\n",
    "        except Exception:\n",
    "            errorCode = 7\n",
    "        \n",
    "        statusMessage = \"pass\"\n",
    "        return statusMessage\n",
    "\n",
    "\n",
    "    def getPaginationURLS(product, base_query, queryHeader, path, payload, productString, pathDelimiter):\n",
    "        #run initial query & get JSON contents\n",
    "\n",
    "        if payload != \"\":\n",
    "            query = requests.post(base_query, headers=queryHeader, data=payload)\n",
    "        else:\n",
    "            query = requests.get(base_query, headers=queryHeader)\n",
    "\n",
    "        #If you are not using the configparser approach, uncomment the next If statement and avoid calling \"updateAccessToken\" method above\n",
    "        if ((query.status_code == 400) or (query.status_code == 401)):\n",
    "            print('need to update access_token')\n",
    "            access_token = updateAccessToken()\n",
    "            newQuery = \"\"\n",
    "            if payload != \"\":\n",
    "                #queryHeader.update({'Content-Type':'application/json'})\n",
    "                queryHeader = {'Accept': 'application/json','Authorization': access_token,'Content-Type': 'application/json'}\n",
    "                newQuery = requests.post(base_query, headers=queryHeader, data=payload)\n",
    "                query = newQuery\n",
    "            else:\n",
    "                queryHeader = {'Accept': 'application/json','Authorization': access_token}\n",
    "                newQuery = requests.get(base_query, headers=queryHeader)\n",
    "                query = newQuery\n",
    "            \n",
    "            if newQuery.status_code != 200:\n",
    "                print(\"Something not working.\")\n",
    "                return False\n",
    "    \n",
    "        # Convert response to json\n",
    "        try:\n",
    "            res = query.json()\n",
    "        except:\n",
    "            if query.status_code == 200:\n",
    "                print('No records found. exiting')\n",
    "            else:\n",
    "                print(query.status_code)\n",
    "                print('Stopping now.')\n",
    "    \n",
    "        productVal =  product\n",
    "        \n",
    "        # Keep the labels for products like drugeventprofile or drugprogrammeaggregationcompanycountry\n",
    "        if productString != \"\":\n",
    "            productVal = productString\n",
    "    \n",
    "        # Define First record\n",
    "        file = str(path) + str(pathDelimiter) + str(productVal) + '-export-pg-1.json'\n",
    "    \n",
    "        with open(file, 'w') as outfile:\n",
    "            # Put the first record into JSON format\n",
    "            json.dump(query.json(), outfile, separators=(', ', ': '), indent=2)\n",
    "        try:\n",
    "            # Check if pagination exists\n",
    "            res['pagination']\n",
    "            nextPage = res['pagination']['nextPage']\n",
    "        except:\n",
    "    \n",
    "            if query.status_code != 200:\n",
    "                print('Possible Startup Error - check credentials and auth_token')\n",
    "    \n",
    "        # We'll store each pagination block into a single array\n",
    "        \n",
    "        myUrls = []\n",
    "        i = 0\n",
    "        print(\"Requests are processing now...\")\n",
    "        \n",
    "        try:\n",
    "            while nextPage:\n",
    "                i += 1\n",
    "                try:\n",
    "                    if payload != \"\":\n",
    "                                \n",
    "                        #queryHeader.append({'Content-Type':'application/json'})\n",
    "                        queryHeader.update({'Content-Type':'application/json'})\n",
    "                        \n",
    "                        query = requests.post(nextPage, headers=queryHeader, data=payload)\n",
    "                    else:\n",
    "                        query = requests.get(nextPage, headers=queryHeader)\n",
    "                    res = query.json()\n",
    "    \n",
    "                    # In rare cases, you may encounter a bad response from the API server.\n",
    "                    # When this occurs, the outage is expected to be less than 5 minutes (300 seconds)\n",
    "                    # The error trap below can keep your process alive for 3 attempts\n",
    "                    j = 0\n",
    "    \n",
    "                    while query.status_code == 500:\n",
    "                        print('internal server error 500 - retrying again in 5 minutes')\n",
    "                        j += 1\n",
    "                        time.sleep(300)\n",
    "                        print('Retrying now')\n",
    "                        if payload != \"\":\n",
    "                            query = requests.post(nextPage, headers=queryHeader, data=payload)\n",
    "                        else:\n",
    "                            query = requests.get(nextPage, headers=queryHeader)\n",
    "                        res = query.json()\n",
    "                        if j == 3:\n",
    "                            print(\"3 unsuccessful attempts. Shutting down now. Please contact PharmaAPIFeedback@informa.com to report an outage\")\n",
    "                            break\n",
    "    \n",
    "                    #copy pagination files to directory with unique name\n",
    "                    paginationFile = path + pathDelimiter + productVal + '-export' + \"-pg-\" + str(i+1) + \".json\"\n",
    "                    with open(paginationFile, 'w') as outfile:\n",
    "                        json.dump(query.json(), outfile, separators=(', ', ': '), indent=2)\n",
    "                    if not res['pagination']:\n",
    "                        print('ok move on')\n",
    "                    if res['pagination']:\n",
    "                        nextPage = res['pagination']['nextPage']\n",
    "                        myUrls.append(nextPage)\n",
    "    \n",
    "                except Exception:\n",
    "    \n",
    "                    print('ok move on')\n",
    "                    base_query_count = base_query + '/count'\n",
    "                    if payload != \"\":\n",
    "                        \n",
    "                        query = requests.post(base_query, headers=queryHeader, data=payload)\n",
    "                    else:\n",
    "                        query = requests.get(base_query_count, headers=queryHeader)\n",
    "                    res = query.json()\n",
    "                    try:\n",
    "                        numRecordsExpected = res['totalRecordCount']\n",
    "                        pagesExpected = numRecordsExpected/100 + 1\n",
    "                        print('Expected number of JSON: ' + str(pagesExpected) + '. Check output directory to confirm: ' + path)\n",
    "                    except Exception:\n",
    "                        errorCode=0\n",
    "                    break\n",
    "        except Exception:\n",
    "            errorCode=0\n",
    "        statusMessage = \"pass\"\n",
    "        return statusMessage\n",
    "    \n",
    "    \n",
    "    def buildPaths(paths, pathDelimiter, outputFormat):\n",
    "        # Whether you are picking up the changes, or getting the entire feed, make a new folder to store your output\n",
    "        pathsTemp = []\n",
    "        for p in paths:\n",
    "            pCount = 0\n",
    "\n",
    "            # Create a new set of folders in case the directory already exists\n",
    "            if os.path.exists(p) == True:\n",
    "                try:\n",
    "                    print(p)\n",
    "                    print(p[:-1])\n",
    "\n",
    "                    newPath = p[:-1] + \"-\" +str(datetime.now().strftime(\"%H%M\") + pathDelimiter)\n",
    "                    if outputFormat == 'xml':\n",
    "                        newPath = p[:-1] + \"-\" +str(datetime.now().strftime(\"%H%M\") + 'xml' + pathDelimiter)\n",
    "                    print('path already exists - trying a different path name: ' + newPath)\n",
    "                    pathsTemp.append(newPath)\n",
    "                    pCount += 1\n",
    "                    os.mkdir(newPath)\n",
    "                except OSError:\n",
    "                    print (\"Creation of the directory %s failed\" % newPath)\n",
    "\n",
    "            elif os.path.exists(p) == False:\n",
    "                try:\n",
    "                    os.mkdir(p)\n",
    "                except OSError:\n",
    "                    print (\"Creation of the directory %s failed\" % p)\n",
    "\n",
    "        if len(pathsTemp) > 0:\n",
    "            paths = pathsTemp\t\n",
    "        return paths \n",
    "\n",
    "\n",
    "    def runQueries(base_path, typeOfRequest, product, payload, changeDate, outputFormat, pathDelimiter):\n",
    "        # Use today's date when naming the folder to store content\n",
    "        todayUTCdate = datetime.now().strftime(\"%Y%m%d\")\n",
    "        path= base_path + pathDelimiter + product + pathDelimiter\n",
    "        paths = [path]\n",
    "\n",
    "        # By default, the script pulls from the feed endpoint\n",
    "        queryType = \"feed\"\n",
    "        queryHeader = {'Accept': 'application/json','Authorization': access_token}\n",
    "\n",
    "        # The next two blocks create folders to store JSON output\n",
    "        changeString = \"\"\n",
    "        year = changeDate[0]\n",
    "        month = changeDate[1]\n",
    "        day = changeDate[2]\n",
    "        changeDate = str(year) + \"-\" + str(month) + \"-\" + str(day)\n",
    "        quickDate = str(year)+str(month)+str(day)\n",
    "        pathToRemovedRecords = \"\"\n",
    "\n",
    "\n",
    "        if 'change' in typeOfRequest.lower():\n",
    "            changeString = \"/changes?since=\" + str(changeDate) + \"&type=remove\"\n",
    "            changesPath = base_path + pathDelimiter + product + \"Changes\" + str(quickDate) + \"to\" + str(todayUTCdate)  \n",
    "            paths = []\n",
    "            paths.append(changesPath)\n",
    "            pathToRemovedRecords = base_path + pathDelimiter + product + \"RemovedRecords\" + str(quickDate) + \"to\" + str(todayUTCdate)  \n",
    "            paths.append(pathToRemovedRecords)\n",
    "\n",
    "        paths = buildPaths(paths, pathDelimiter, outputFormat)\n",
    "\n",
    "        # The next section is specific to Biomedtracker, Datamonitor Healthcare and Meddevicetracker\n",
    "        subEndpoint = \"\"\n",
    "        newVars = checkSubEndpoint(product)\n",
    "\n",
    "        # Use productString to retain sub-endpoint combinations (e.g. drugeventprofile) - leveraged for file names\n",
    "        productString = \"\"\n",
    "\n",
    "        if len(newVars) > 0:\n",
    "            productString = product\n",
    "            product = newVars[0]\n",
    "            subEndpoint = newVars[1]\n",
    "\n",
    "\n",
    "\n",
    "        # Update the query string to pickup 'subEndpoint' if needed\n",
    "        base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint + changeString \n",
    "\n",
    "        # API variable prep complete - ready to run queries\n",
    "        # Run the function that fetches each pagination URL and store into path\n",
    "\n",
    "        if 'xml' in outputFormat.lower():\n",
    "\n",
    "\n",
    "            if typeOfRequest.lower() == 'feedchanges':\n",
    "                print(\"feed changes start\")\n",
    "                # Pickup the latest changes based on date specified above\n",
    "                #queryHeader.update({'Accept':'application/xml'})\n",
    "                #queryHeader.append({'Content-Type':'application/json'})\n",
    "\n",
    "                queryHeader = {'Accept': 'application/json','Authorization': access_token,'Content-Type':'application/json'}\n",
    "\n",
    "                queryType = \"search\"\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint\n",
    "\n",
    "                payload = \"{\\\"and\\\": [\\n        {\\n            \\\"gte\\\":{\\n                \\\"value\\\": \\\"\" + str(year) +\"-\"+str(month)+\"-\"+str(day)+\"\\\",\\n                \\\"name\\\": \\\"updatedDate\\\"\\n            }\\n        }\\n        ]\\n} \\n\\n\"\n",
    "                # Store changes since date\n",
    "                print(\"Storing the latest changes\")\n",
    "                getXMLPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "\n",
    "                # Document records that no longer exist within web products\n",
    "                print(\"storing records to remove\")\n",
    "                payload=\"\"\n",
    "                queryType = \"feed\"\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint + changeString\n",
    "                print(base_query)\n",
    "                del queryHeader['Content-Type']\n",
    "\n",
    "                try:\n",
    "                    path = paths[1]\n",
    "                except:\n",
    "                    path = paths[0]\n",
    "                getXMLPaginationURLS(product, base_query, queryHeader, path, payload, productString, pathDelimiter)\n",
    "\n",
    "\n",
    "            elif typeOfRequest.lower() == 'feed':\n",
    "                print(\"MADE IT HERE!\")\n",
    "                payload = ''\n",
    "                queryHeader = {'Accept': 'application/xml','Authorization': access_token}\n",
    "                getXMLPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "        else:\n",
    "            if 'adhoc' in typeOfRequest.lower():\n",
    "                queryType = \"search\"\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint\n",
    "                if payload != \"\":\n",
    "                    print(\"queryHeader before: \" + str(queryHeader))\n",
    "                    print(\"\")\n",
    "                    #queryHeader.append({'Content-Type':'application/json'})\n",
    "                    queryHeader.update({'Content-Type':'application/json'})\n",
    "                    print(\"queryHeader after: \" + str(queryHeader))\n",
    "                print(\"Storing json from ad-hoc query\")\n",
    "                getPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "            elif 'change' in typeOfRequest.lower():\n",
    "                # Pickup the latest changes based on date specified above\n",
    "                queryHeader.update({'Content-Type':'application/json'})\n",
    "\n",
    "                queryType = \"search\"\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint\n",
    "\n",
    "                payload = \"{\\\"and\\\": [\\n        {\\n            \\\"gte\\\":{\\n                \\\"value\\\": \\\"\" + str(year) +\"-\"+str(month)+\"-\"+str(day)+\"\\\",\\n                \\\"name\\\": \\\"updatedDate\\\"\\n            }\\n        }\\n        ]\\n} \\n\\n\"\n",
    "                # Store changes since date\n",
    "                print(\"Storing the latest changes\")\n",
    "                getPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "                # Document records that no longer exist within web products\n",
    "                print(\"storing records to remove\")\n",
    "                payload=\"\"\n",
    "                queryType = \"feed\"\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint + changeString\n",
    "                print(base_query)\n",
    "                del queryHeader['Content-Type']\n",
    "\n",
    "                try:\n",
    "                    path = paths[1]\n",
    "                except:\n",
    "                    path = paths[0]\n",
    "                getPaginationURLS(product, base_query, queryHeader, path, payload, productString, pathDelimiter)\n",
    "\n",
    "            elif 'schema' in typeOfRequest.lower():\n",
    "                queryType = 'search'\n",
    "                payload = \"\"\n",
    "                subEndpoint = subEndpoint + \"/schema\"\n",
    "\n",
    "                base_query = 'https://api.pharmaintelligence.informa.com/v1/' + queryType + \"/\" + product + subEndpoint\n",
    "                getPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "            else:\n",
    "                # Get entire feed endpoint\n",
    "                payload = \"\"\n",
    "                getPaginationURLS(product, base_query, queryHeader, paths[0], payload, productString, pathDelimiter)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        typeOfRequest = 'adhoc' #\"adhoc\" \"feed\" #schema or #\"feedChanges\"\n",
    "        outputFormat = 'json' #default is JSON, switch to xml if needed \n",
    "\n",
    "        #Define path for JSON Files - this path must exist before running\n",
    "        base_path = 'C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial'\n",
    "        osType = 'windows' # other options are mac or linux\n",
    "\n",
    "        #Set which API\n",
    "        product = 'trial'\n",
    "        ''' other products include:  drug, investigator, organization, trial, drugcatalyst,drugcatalysttimeseries,drugevent,drugeventprofile, forecastconsensusdrug, forecastpharmavitaecompany, forecastpharmavitaedrug, devicecatalyst, deviceevent, deviceproduct, devicetrial'''\n",
    "\n",
    "        # If you selected 'feedChanges' Set date of the last time you ran an update. The script will then pick up the changes from a specific date\n",
    "        year = '2020'\n",
    "        month = '02'\n",
    "        day = '10'\n",
    "\n",
    "        #### End EDIT #####\n",
    "\n",
    "        # Next block handles the folder paths based on OS type\n",
    "        pathDelimiter = str('\\\\')\n",
    "\n",
    "        if osType.lower() != 'windows':\n",
    "            pathDelimiter = str('/')\n",
    "\n",
    "        # If the changes endpoint is selected, the changeDate will be used\n",
    "        changeDate = [year, month, day]\n",
    "\n",
    "\n",
    "        # Ad-Hoc queries - copy and paste directly from Postman\n",
    "        #payload = \"\"\n",
    "        payload = \"{\\n\\t\\n \\\"and\\\":[\\n\\t{\\n      \\\"is\\\": {\\n            \\\"value\\\": [368929,370564,370526,370481,370540,369754,369530,370556,370240,369886,369561,369568,370521,370049,369727,370474,370050,368659,370272,369664,370220,370038,367951,367949,369845,368233,370553,370278,370379,370242,368855,369000,367765,367955,370375,370293,367051,369376,368092,370635,370524,369993,369962,369451,369095,370632,368096,369585,367945,369593,369949,370580,368569,370378,370583,370434,367608,369750,366817,370091,369917,370096,369941,367947,367003,367147,366938,366942,368066,370584,367291,369921,367299,265491,368935,370566,370333,368231,368258,370432,370475,370499,367412,369074,370069,370423,368934,370334,369238,370646,370504,370578,370651,370430,367233,370638,368058,368935,367042,368842,369627,370507,370493,370492,370518,368312,367254,367584,370538,367420,369446,370477,368091,366740,311873,370530,370139,367190,366999,367143,367007,368061,370194,369329,368243,366283,370262,368053,333420,369838,216634,368073,370816,370799,370894,370797,370814,370794,370821,370897,370895,370823,370907,370819,370906,370889,370903,370808,370839,370921,370905,350511,370884,370802,370892,370832,370805,370890,370896],\\n            \\\"name\\\": \\\"trialId\\\"\\n          }\\n     }\\n\\n]}\\n\\n\"\n",
    "\n",
    "        #import argparse\n",
    "\n",
    "        #myArgParser = argparse.ArgumentParser()\n",
    "        #myArgParser.add_argument('--product', nargs='*', action='append', dest='alist')\n",
    "\n",
    "        #myArgParser.add_argument('--format', nargs='*', action='append', dest='alist2')\n",
    "\n",
    "\n",
    "        #args = myArgParser.parse_args()\n",
    "        #print(args)\n",
    "\n",
    "\n",
    "        #productList = args.alist\n",
    "\n",
    "        #formatList = args.alist2\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(productList[0])\n",
    "            product = productList[0][0]\n",
    "\n",
    "        except Exception:\n",
    "            print('No args supplied at run-time. We will use value supplied in script')\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(formatList[0])\n",
    "            outputFormat = formatList[0][0]\n",
    "\n",
    "        except Exception:\n",
    "            print('No args supplied at run-time. We will use value supplied in script')\n",
    "\n",
    "\n",
    "        print(product)\n",
    "        runQueries(base_path, typeOfRequest, product, payload, changeDate,outputFormat, pathDelimiter)\n",
    "\n",
    "        print(\"Done\")\n",
    "\n",
    "    #=====#\n",
    "\n",
    "    #2\n",
    "\n",
    "    import pandas as pd\n",
    "    from pandas.io.json import json_normalize\n",
    "    import numpy as np\n",
    "    import os, json, time\n",
    "    from datetime import date, datetime #, timezone\n",
    "    todayUTCdateTime = datetime.utcnow().strftime(\"%Y%m%d%H%M\")\n",
    "    import csv #Used to save as CSV\n",
    "    from sqlalchemy import create_engine\n",
    "    import openpyxl\n",
    "\n",
    "    def change_column_order(df, col_name, index):\n",
    "        cols = df.columns.tolist()\n",
    "        cols.remove(col_name)\n",
    "        cols.insert(index, col_name)\n",
    "        return df[cols]\n",
    "\n",
    "    def read_input(inputListPath):\n",
    "        #Assumes you have a header row in your CSV file\n",
    "        inputList = []\n",
    "        headerRow = []\n",
    "        with open(inputListPath, 'r',encoding='utf-8-sig') as csvfile:\n",
    "            numRows = 0\n",
    "            for row in csv.reader(csvfile.read().splitlines()):\n",
    "                if numRows == 0:\n",
    "                    headerRow = row\n",
    "                if numRows != 0:\n",
    "                    inputList.append(row)\n",
    "                numRows += 1\n",
    "            #print('Processing ' + str(len(inputList)) + ' records now. Do not close this window...')\n",
    "\n",
    "\n",
    "        return inputList, headerRow\n",
    "\n",
    "    def getPrimaryKey(product):\n",
    "        #newVars = []\n",
    "        primaryKey = product + \"Id\"\n",
    "        apiEndpointMap, apiEndpointMapHeaders = read_input('./primaryKeyMap.csv')\n",
    "\n",
    "        for item in apiEndpointMap:\n",
    "\n",
    "            if product == item[0]:\n",
    "                primaryKey = item[1] + \"Id\"\n",
    "\n",
    "        return primaryKey\n",
    "\n",
    "    def getMetadataById(myReportId):\n",
    "\n",
    "        #if a mapKeys 'sendOutput' is tagged with \"y\" then we will include it \n",
    "        reportMap, reportMapCols = read_input('./jsonETLmap.csv')\n",
    "        reportMapDF = pd.DataFrame(data=reportMap,columns=reportMapCols).set_index('reportId')\n",
    "\n",
    "        print('Report to be processed based on supplied metadata:')\n",
    "        inputMetadata = reportMapDF[reportMapDF.index == myReportId]\n",
    "        print(inputMetadata)\n",
    "        return inputMetadata\n",
    "\n",
    "\n",
    "    def getMetadata():\n",
    "        #if a mapKeys 'sendOutput' is tagged with \"y\" then we will include it \n",
    "        reportMap, reportMapCols = read_input('./jsonETLmap.csv')\n",
    "        reportMapDF = pd.DataFrame(data=reportMap,columns=reportMapCols).set_index('reportId')\n",
    "        inputMetadata = reportMapDF[reportMapDF.sendOutput == 'y']\n",
    "        return inputMetadata\n",
    "\n",
    "\n",
    "    def getMetadataOld():\n",
    "\n",
    "        reportMap, reportMapCols = read_input('./jsonETLmap.csv')\n",
    "\n",
    "        #if a mapKeys 'sendOutput' is tagged with \"y\" then we will include it \n",
    "        reportMapKeys, reportMapKeysCols = read_input('./jsonETLmapKeys.csv')\n",
    "\n",
    "        reportMapKeysDF = pd.DataFrame(data=reportMapKeys,columns=reportMapKeysCols).set_index('reportTypeID')\n",
    "        reportMapDF = pd.DataFrame(data=reportMap,columns=reportMapCols).set_index('reportTypeID')\n",
    "\n",
    "        reportsTracker = reportMapDF.join(reportMapKeysDF,how='left')\n",
    "        inputMetadata = reportsTracker[reportsTracker.sendOutput == 'y']\n",
    "\n",
    "        return inputMetadata\n",
    "\n",
    "    def getNumFiles(filenames):\n",
    "        numFiles = 0\n",
    "        for file in filenames:\n",
    "            numFiles += 1\n",
    "        return numFiles\n",
    "\n",
    "    def getJSON(path):\n",
    "        '''Loop through each JSON file and add JSON to an array. Pass the array back to main'''\n",
    "        filenames = os.listdir(path)\n",
    "        recordHolder = []\n",
    "        totalNumFiles = getNumFiles(filenames)\n",
    "\n",
    "        recordsPerFile = 100\n",
    "\n",
    "        numFiles = 0\n",
    "        print('Loading JSON from ' + str(totalNumFiles) + ' total records - do not close this window...')\n",
    "        for file in filenames:\n",
    "            numFiles += 1\n",
    "            filename = path + file\n",
    "\n",
    "            #Load each JSON file and store into an Array\n",
    "            with open(filename,\"r\") as f:\n",
    "                jsondata=json.load(f)\n",
    "\n",
    "                try:\n",
    "                    # By default, informa API delivers data in pagination blocks.\n",
    "                    # Each pagination block has 100 'items'\n",
    "                    recordHolder.append(jsondata['items'])\n",
    "                except:\n",
    "                    # Alternate implementation - use if json not wrapped in 'items' tag -\n",
    "                    recordHolder.append(jsondata)\n",
    "                #Optional section - Keep track of records and print output\n",
    "                blockSize = 500\n",
    "                if (numFiles % blockSize == 0):\n",
    "                    print('Processed ' + str(numFiles*100) + ' records from local directory so far. Do not close this window...')\n",
    "            f.close()\n",
    "        print('Data load complete. Processed ' + str(numFiles*100) + ' records from local directory')\n",
    "        return recordHolder\n",
    "\n",
    "\n",
    "    def getJSONslim(inputData, colArray, primaryKey):\n",
    "        #if a mapKeys 'sendOutput' is tagged with \"y\" then we will include it \n",
    "        #reportMap, reportMapCols = read_input('./jsonETLmap.csv')\n",
    "        #primaryKey = getPrimaryKey(product)\n",
    "        dfCols = [primaryKey]\n",
    "\n",
    "        #b = np.array([inputData])\n",
    "        #transposedInputData = b.T\n",
    "\n",
    "        for col in colArray:\n",
    "            dfCols.append(col)\n",
    "        slimJSONdf = pd.DataFrame(data=inputData, columns=dfCols)#.set_index(primaryKey)\n",
    "        #reportMapDF = pd.DataFrame(data=reportMap,columns=colArray).set_index('reportTypeID')\n",
    "        #inputMetadata = reportMapDF[reportMapDF.sendOutput == 'y']\n",
    "        return slimJSONdf\n",
    "\n",
    "\n",
    "    def getFields(product,src,dpth = 0, key = '', keys=None):\n",
    "        if keys is None:\n",
    "            primaryKey = getPrimaryKey(product)\n",
    "            keys = [primaryKey]\n",
    "        if isinstance(src, dict):\n",
    "            for key, value in src.items():\n",
    "                print(key)\n",
    "                if not key in keys:\n",
    "                    keys.append(key)\n",
    "                getFields(product, value, dpth + 1, key, keys)\n",
    "        return keys\n",
    "\n",
    "    def getObjectColumns(src, keys):\n",
    "\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "\n",
    "        if isinstance(src, list):\n",
    "\n",
    "            for parentKey in src:\n",
    "\n",
    "                    for key in parentKey:\n",
    "                        #print(key)\n",
    "                        for val in key:\n",
    "                            if key not in keys:\n",
    "                                keys.append(key)\n",
    "                        #if isinstance(key, dict) or isinstance(key, list):\n",
    "                            #print(key)\n",
    "\n",
    "                        #\n",
    "                    #for key, value in parentKey.items():\n",
    "\n",
    "        return keys\n",
    "\n",
    "    def flattenRecordsWithLists(srcData, primaryKey, queryData, queryObjectColumns, subKeys, subSubKeys,colArray,listCols):\n",
    "\n",
    "\n",
    "\n",
    "        records = []\n",
    "        for record in srcData:\n",
    "            #First, check for dictionary objects\n",
    "            #Second, check list objects\t#Check for dictionary objects within the list\n",
    "            #Lastly, get the objects that sit at the top of the JSON hierarchy and do not contain dictionary/list objects\n",
    "            #Rename keys that can cause issues\n",
    "\n",
    "            tempRecords = []\n",
    "            #flatRecordTemp = record\n",
    "\n",
    "            normalized_data_temp = pd.DataFrame()\n",
    "            listDFs = []\n",
    "\n",
    "            j = 0\n",
    "\n",
    "            while j < len(listCols):\n",
    "\n",
    "                listKey = listCols[j]\n",
    "\n",
    "                #use the json_normalize function to scan the input json (trialDataTemp) with inputCols\n",
    "                normalized_data_temp = json_normalize(record, listKey,[primaryKey])\n",
    "\n",
    "\n",
    "                listKeyDF = (pd.concat({i: json_normalize(x) for i, x in normalized_data_temp.pop(listKey).items()},sort=False).reset_index(level=1, drop=True))\n",
    "\n",
    "                #print(listKeyDF)\n",
    "\n",
    "                # Clean up flat structure - avoid using duplicate keys for \"id\" \"name\" etc.\n",
    "                listKeyDF.rename(columns={'id': listKey+'Id', 'name': listKey}, inplace=True)\n",
    "\n",
    "                #join flatTrialDiseases with step above\n",
    "                normalized_data_temp = (listKeyDF.join(normalized_data_temp).reset_index(drop=True))\t\n",
    "\n",
    "                normalized_data = normalized_data_temp.set_index(primaryKey)\n",
    "                tempRecords.append(normalized_data)\t\n",
    "\n",
    "\n",
    "            # Combine flat columns with data that we just flattened\n",
    "            firstPieceDFTemp = pd.DataFrame(data=[record], columns=listCols)\n",
    "            firstPieceDF = firstPieceDFTemp.set_index(primaryKey)\n",
    "            nextPieceDF = pd.DataFrame()\n",
    "            nextPieceDF = firstPieceDF.join(tempRecords)\n",
    "\n",
    "            # Save combined record - loop onto the next record\n",
    "            records.append(nextPieceDF)\n",
    "\n",
    "        return records\n",
    "\n",
    "\n",
    "    def flattenRecords(srcData, primaryKey, queryData, queryObjectColumns, subKeys, subSubKeys,colArray,listCols):\n",
    "\n",
    "\n",
    "\n",
    "        records = []\n",
    "        for record in srcData:\n",
    "            #First, check for dictionary objects\n",
    "            #Second, check list objects\t#Check for dictionary objects within the list\n",
    "            #Lastly, get the objects that sit at the top of the JSON hierarchy and do not contain dictionary/list objects\n",
    "            #Rename keys that can cause issues\n",
    "\n",
    "            tempRecords = []\n",
    "            #flatRecordTemp = record\n",
    "\n",
    "            normalized_data_temp = pd.DataFrame()\n",
    "            listDFs = []\n",
    "\n",
    "            j = 0\n",
    "\n",
    "            while j < len(queryObjectColumns):\n",
    "\n",
    "\n",
    "                #use the json_normalize function to scan the input json (trialDataTemp) with inputCols\n",
    "                normalized_data_temp = json_normalize(record, queryObjectColumns[j],[primaryKey])\n",
    "\n",
    "\n",
    "                ### TO DO - 'catalystAnalysis.source contains duplicate 'id' keys \n",
    "                if 'source' in normalized_data_temp.columns:\n",
    "                    normalized_data_temp2 = normalized_data_temp.drop(columns=['source'])\n",
    "                    normalized_data_temp = normalized_data_temp2\n",
    "                #elif 'indications' in normalized_data_temp.columns:\n",
    "                #\tnormalized_data_temp2 = normalized_data_temp.drop(columns=['indications'])\n",
    "                #\tnormalized_data_temp = normalized_data_temp2\n",
    "\n",
    "                #print(normalized_data_temp)\n",
    "\n",
    "                #listCols = ['trialIds','trialId','diseaseHierarchy','ongoingTrials','pastTrials','organizationPrimaryInvestigators','organizationAffiliatedInvestigators','trialInvestigators']\n",
    "                #if queryObjectColumns[j] in listCols:\n",
    "\n",
    "\n",
    "\n",
    "                if queryObjectColumns[j] in listCols:\n",
    "\n",
    "                        #print(normalized_data_temp.columns)\n",
    "\n",
    "                            #print(normalized_data_temp)\n",
    "\n",
    "                        #print(normalized_data_temp)\n",
    "                        #pdListTemp  = pd.DataFrame(data=normalized_data_temp)\n",
    "                        pdList = []\n",
    "\n",
    "                        for listKey in record[queryObjectColumns[j]]:\n",
    "                            pdDict = {}\n",
    "                            recordKey = record[primaryKey]\n",
    "                            pdDict[primaryKey] = recordKey\n",
    "\n",
    "                            pdDict[queryObjectColumns[j]] = listKey\n",
    "\n",
    "                            pdList.append(pdDict)\n",
    "\n",
    "\n",
    "                        dictKeeper = {}\n",
    "\n",
    "                        #print(pdList)\n",
    "\n",
    "\n",
    "                        contextData = pd.DataFrame.from_dict(pdList)\n",
    "                        #if queryObjectColumns[j] not in queryData:\n",
    "                                #queryData.append(queryObjectColumns[j])\n",
    "\n",
    "                        if len(contextData) > 0:\n",
    "                            normalized_data_temp = contextData\n",
    "\n",
    "                try:\n",
    "                    for subKey in subKeys:\n",
    "\n",
    "                        if subKey in listCols:\n",
    "\n",
    "\n",
    "                            recordList = record[queryObjectColumns[j]]\n",
    "                            #print(queryObjectColumns[j])\n",
    "                            #print(subKey)\n",
    "\n",
    "                            #if subKey not in queryData:\n",
    "                            #\tif queryObjectColumns[j] not in queryData:\n",
    "                            #\t\tqueryData.append(queryObjectColumns[j])\n",
    "                            #\tqueryData.append(subKey)\n",
    "\n",
    "                            #print(recordList) \n",
    "                            for listKeySet in recordList:\n",
    "\n",
    "                                #print(subKey)\n",
    "                                #print(listKeySet)\t\t\t\t\t\t\t\n",
    "\n",
    "                                if listKeySet.items():\n",
    "                                    for listKey, listKeyVals in listKeySet.items():\n",
    "                                        pdList = []\n",
    "                                        if listKey == subKey:\n",
    "                                            #print(listKey)\n",
    "                                            #print(listKeyVals)\n",
    "                                            for val in listKeyVals:\n",
    "                                                #print(val)\n",
    "\n",
    "                                                pdDict = {}\n",
    "\n",
    "                                                recordLookupKey = record[primaryKey]\n",
    "                                                pdDict[primaryKey] = recordLookupKey\n",
    "\n",
    "                                                #recordKey = listKey\n",
    "                                                #pdDict[listKeyVals] = val\n",
    "\n",
    "                                                pdDict[subKey] = val\n",
    "                                                #pdDict[subKey] = listKey[val]\n",
    "                                                pdList.append(pdDict)\n",
    "\n",
    "\n",
    "                            contextData = pd.DataFrame.from_dict(pdList)\n",
    "\n",
    "                            if len(contextData) > 0:\n",
    "                                normalized_data_temp = contextData\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            subKeysDF = (pd.concat({i: json_normalize(x) for i, x in normalized_data_temp.pop(subKey).items()},sort=False).reset_index(level=1, drop=True))\n",
    "                            # Clean up flat structure - avoid using duplicate keys for \"id\" \"name\" etc.\n",
    "                            subKeysDF.rename(columns={'id': subKey+'Id', 'name': subKey}, inplace=True)\n",
    "\n",
    "                            #join flatTrialDiseases with step above\n",
    "                            normalized_data_temp = (subKeysDF.join(normalized_data_temp).reset_index(drop=True))\n",
    "\n",
    "                except:\n",
    "                    errorcode=0\n",
    "\n",
    "                try:\n",
    "                    for subSubKey in subSubKeys:\n",
    "                        if subSubKey in listCols:\n",
    "\n",
    "                            pdList = []\n",
    "                            parentKey = \"\"\n",
    "                            for listKey in record[queryObjectColumns[j]]:\n",
    "\n",
    "                                recordLookupKey = record[primaryKey]\n",
    "\n",
    "\n",
    "                                recordKey = record[queryObjectColumns[j]]\n",
    "\n",
    "                                for k,v in listKey.items():\n",
    "\n",
    "                                    if k != subSubKey:\n",
    "                                        parentKey = k\n",
    "                                #subSubVals =[]\n",
    "\n",
    "                                #pdDict[subSubKey] = listKey[subSubKey]\n",
    "                                for val in listKey[subSubKey]:\n",
    "                                    pdDict = {}\n",
    "                                    #print(val)\n",
    "\n",
    "                                    #print(listKey[parentKey])\n",
    "                                    #print(str(subSubKey) + \": \" + str(val))\n",
    "                                    #pdDict[subKey] = \n",
    "                                    pdDict[primaryKey] = recordLookupKey\n",
    "                                    pdDict[parentKey] = listKey[parentKey]\n",
    "                                    pdDict[subSubKey] = val\n",
    "                                    #print(parentKey)\n",
    "                                #print(pdDict)\n",
    "\n",
    "                                    pdList.append(pdDict)\n",
    "\n",
    "                            #print(len(pdList))\n",
    "                            if len(pdList) > 0:\n",
    "                                contextDataTemp = pd.DataFrame(data=pdList)\n",
    "                                #contextDataTemp = pd.DataFrame.from_dict(pdList)\n",
    "                                #print(contextDataTemp)\n",
    "\n",
    "                            #if contextDataTemp.size > 0:\n",
    "\n",
    "                                #contextDataTemp2 = contextDataTemp.set_index(primaryKey)\n",
    "                                #print(contextDataTemp2)\n",
    "                                #contextDataTemp5 = pd.DataFrame(data=contextDataTemp2,columns=[parentKey])\n",
    "                                #print(contextDataTemp5)\n",
    "                                #contextDataTemp6 = pd.DataFrame(data=contextDataTemp2,columns=[subSubKey])\n",
    "                                #print(contextDataTemp6)\n",
    "                                #contextData = json_normalize(contextDataTemp5)\n",
    "                                #print(\"Hey\")\n",
    "                                #print(contextData)\n",
    "                                normalized_data_temp = contextDataTemp\n",
    "                                #print(normalized_data_temp)\n",
    "                        else:\t\n",
    "                            subSubKeysDF = (pd.concat({i: json_normalize(x) for i, x in normalized_data_temp.pop(subSubKey).items()},sort=False).reset_index(level=1, drop=True))\n",
    "                            # Clean up flat structure - avoid using duplicate keys for \"id\" \"name\" etc.\n",
    "                            subSubKeysDF.rename(columns={'id': subSubKey+'Id', 'name': subSubKey}, inplace=True)\n",
    "\n",
    "                            #join flatTrialDiseases with step above\n",
    "                            normalized_data_temp = (subSubKeysDF.join(normalized_data_temp).reset_index(drop=True))\t\t\t\n",
    "\n",
    "                except:\n",
    "                    errorCode=0\n",
    "\n",
    "                #print(normalized_data_temp)\n",
    "\n",
    "                # Rename the existing DataFrame columns (rather than creating a copy)\n",
    "                normalized_data_temp.rename(columns={'name': queryObjectColumns[j],'id': queryObjectColumns[j]+\"Id\",'type': queryObjectColumns[j]+\"Type\"}, inplace=True)\n",
    "\n",
    "\n",
    "                normalized_data = normalized_data_temp.set_index(primaryKey)\n",
    "                tempRecords.append(normalized_data)\n",
    "\n",
    "\n",
    "                j += 1\n",
    "\n",
    "            #print(listDFs)\n",
    "            #print(queryData)\n",
    "\n",
    "            # Combine flat columns with data that we just flattened\n",
    "            firstPieceDFTemp = pd.DataFrame(data=[record], columns=queryData)\n",
    "            firstPieceDF = firstPieceDFTemp.set_index(primaryKey)\n",
    "            nextPieceDF = pd.DataFrame()\n",
    "            nextPieceDF = firstPieceDF.join(tempRecords)\n",
    "\n",
    "            #print(colArray)\n",
    "            #if nextPieceDF.columns.size > 0:\n",
    "            #\tnextPieceDF.columns = colArray\n",
    "            #if nextPieceDF.size >= 1:\n",
    "            #\tprint(nextPieceDF)\n",
    "\n",
    "\n",
    "            # Save combined record - loop onto the next record\n",
    "            records.append(nextPieceDF)\n",
    "        #print(records)\n",
    "        return records\n",
    "\n",
    "\n",
    "    def getDF(product, srcData, queryData, objectColumns, subKeys, subSubKeys,colArray,listCols,richTextCols):\n",
    "\n",
    "        primaryKey = getPrimaryKey(product)\n",
    "\n",
    "        objectColumnBatch = objectColumns\n",
    "\n",
    "        #############\n",
    "        #if richTextCols != ['']:\n",
    "            #flattestJSONs = pd.DataFrame(data=[primaryKey,richTextCols],columns=[primaryKey,richTextCols])\n",
    "            #primaryKeys = pd.DataFrame(data=[primaryKey])\n",
    "            #print(primaryKeys)\n",
    "            #flattestJSONs = pd.DataFrame(data=[primaryKey,richTextCols])\n",
    "\n",
    "        ########\n",
    "        #elif objectColumns != ['']:\n",
    "\n",
    "        # Ensure 'queryData' does not contain columns we'll pull later in the process\n",
    "        n = 0\n",
    "        while n < len(objectColumnBatch):\n",
    "            try:\n",
    "                #print(objectColumnBatch[n])\n",
    "                queryData.remove(objectColumnBatch[n])\n",
    "            except:\n",
    "                errorCode=0\n",
    "            n += 1\n",
    "\n",
    "        #print(colArray)\n",
    "        #print(objectColumns)\n",
    "        inputArray = [srcData]\n",
    "        #print(inputArray[0][objectColumns[0]])\n",
    "        #slimInputArray = inputArray[0][objectColumns[0]]\n",
    "        #print(slimInputArray)\n",
    "        #slimJSONdf = getJSONslim(inputArray, colArray, primaryKey)\n",
    "        #print(slimJSONdf)\n",
    "        #slimInputArray = slimJSONdf.to_numpy()\n",
    "\n",
    "\n",
    "        #print(len(richTextCols))\n",
    "        result = pd.DataFrame()\n",
    "        #try:\n",
    "        if len(richTextCols) > 0: # != [''] and richTextCols != []:\n",
    "\n",
    "            resultTemp = pd.DataFrame(data=inputArray,columns=[primaryKey,richTextCols])\n",
    "            result = resultTemp.set_index(primaryKey)\n",
    "            #print(result)\n",
    "        else:\n",
    "            records = flattenRecords(inputArray, primaryKey,queryData, objectColumnBatch, subKeys, subSubKeys,colArray,listCols)\n",
    "\n",
    "            # Combine records into single dataframe\n",
    "            result = pd.concat(records, sort=False)\n",
    "        #print(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def buildPartitions(product, srcArray, queryData, objectColumns, subKeys, subSubKeys,colArray,listCols,richTextCols):\n",
    "\n",
    "\n",
    "        #JSON currently is stored within many arrays. The next lines pull json up a level\n",
    "        srcJSONs = []\n",
    "\n",
    "        for src in srcArray:\n",
    "            for s in src:\n",
    "                srcJSONs.append(s)\n",
    "\n",
    "        #Optimize the runtime of this script by setting the block size \n",
    "        #Rather than processing all records at once, break into blocks\n",
    "        print(\"Number of Records: \" + str(len(srcJSONs)))\n",
    "        numSrcJSONs = len(srcJSONs)\n",
    "        blockSize = 200\n",
    "        print(\"Block Size = \" + str(blockSize))\n",
    "        if len(srcJSONs) < blockSize:\n",
    "            blockSize=len(srcJSONs)\n",
    "\n",
    "        #Esnure the number of loops matches the record set\n",
    "        numBlocks = int(len(srcJSONs)/blockSize)\n",
    "        #The final block will be smaller than the other blocks. \n",
    "        diff = len(srcJSONs) - (numBlocks * blockSize)\n",
    "\n",
    "        #Scan the array and put the final block into a separate array\n",
    "        lenSRCJSON = len(srcJSONs)\n",
    "        lastTotalBlockSize = diff\n",
    "        lastTotalBlock = srcJSONs[lenSRCJSON - diff:]\n",
    "\n",
    "        #If you select a blocksize that results in 0 blocks, we ensure the looping picks up every record\n",
    "        if numBlocks==0:\n",
    "            blockSize=len(srcJSONs)\n",
    "            lastTotalBlock=[srcJSONs]\n",
    "        print(\"Last Total Block Size = \" + str(lastTotalBlockSize))\n",
    "\n",
    "        print('Attempting to process : ' + str(numBlocks) + \" blocks of \" + str(blockSize) +\" records\")\n",
    "        flattestJSONs = pd.DataFrame()\n",
    "\n",
    "        flatJSONs = []\n",
    "        blockNum = 0\n",
    "\n",
    "\n",
    "        while blockNum < numBlocks:\n",
    "            srcJSONBlock = []\n",
    "            x = 0\n",
    "            #Using the blocksize specified above, jump to the next set of records and store into a temporary array\n",
    "            while x < blockSize:\n",
    "                srcJSONBlock.append(srcJSONs[(blockNum*blockSize)+x])\n",
    "                x += 1\n",
    "\n",
    "            #Leveraging the array above, flatten each array element via \"recursiveKeyLookup\" function\n",
    "            y = 0\n",
    "\n",
    "            while y < len(srcJSONBlock):\n",
    "                #print(objectColumns)\n",
    "                #print(len(objectColumns))\n",
    "\n",
    "                flatJSON = getDF(product, srcJSONBlock[y], queryData, objectColumns, subKeys, subSubKeys,colArray,listCols,richTextCols)\n",
    "                #print(flatJSON)\n",
    "                #flatJSON = recursiveKeyLookup(srcJSONBlock[y])\n",
    "                flatJSONs.append(flatJSON)\n",
    "                y += 1\n",
    "\n",
    "            #Optional progress checker\n",
    "            if (blockNum % 100) == 0:\n",
    "                print('Flattened ' + str(blockNum) + ' records. Do not close this window...')\t\t\n",
    "\n",
    "            blockNum += 1\n",
    "\n",
    "        print(\"Length of flatJSONs before extra sets added = \" + str(len(flatJSONs)))\n",
    "\n",
    "        #Add the remaining values\n",
    "        k = 0\n",
    "        while k < len(lastTotalBlock):\n",
    "\n",
    "            moreFlatJSON = getDF(product, lastTotalBlock[k], queryData, objectColumns, subKeys, subSubKeys,colArray,listCols,richTextCols)\n",
    "\n",
    "            flatJSONs.append(moreFlatJSON)\n",
    "            k += 1\n",
    "\n",
    "        print(\"Done processing blocks\")\n",
    "        print(\"Length of flatJSONs after extra sets added = \" + str(len(flatJSONs)))\n",
    "        flattestJSONs = pd.concat(flatJSONs, sort=False)\n",
    "        #print(flattestJSONs)\n",
    "\n",
    "\n",
    "\n",
    "        return flattestJSONs\n",
    "\n",
    "\n",
    "    def cleanupDF(df, product):\n",
    "        #print(df.columns)\n",
    "\n",
    "\n",
    "        print(\"Removing non UTF-8 characters\")\n",
    "        #Non UTF-8 characters can exist within data set. The next line eliminates any upload errors for CSV & SQLITE\n",
    "        dfCleaned = df.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "        #Set variable that will be leveraged by various export functions\n",
    "        df_toExport = dfCleaned\n",
    "\n",
    "        return df_toExport\n",
    "\n",
    "\n",
    "    def cleanDoubleQuotes(value):\n",
    "        print(type(value))\n",
    "\n",
    "        valueCleaned = value.applymap(lambda x: x.replace('\"','') if isinstance(x, str) else x)\n",
    "        print(valueCleaned)\n",
    "        #return value.replace('\"\"', '')\n",
    "        return valueCleaned\n",
    "\n",
    "    def createOutput(product, inputData, queryData, objectColumns, subKeys, subSubKeys,colArray,reportType,base_path,listCols,richTextCols, pathDelimiter,date):\n",
    "        print(richTextCols)\n",
    "\n",
    "\n",
    "        myDFTemp = buildPartitions(product, inputData, queryData, objectColumns, subKeys, subSubKeys,colArray,listCols,richTextCols)\n",
    "        #myDF = cleanupDF(myDFTemp, product)\n",
    "        #print(myDFTemp)\n",
    "\n",
    "        #print(\"Removing non UTF-8 characters\")\n",
    "        #Non UTF-8 characters can exist within data set. The next line eliminates any upload errors for CSV & SQLITE\n",
    "        #dfCleaned = myDFTemp.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "        myDFTemp2 = cleanDoubleQuotes(myDFTemp)\n",
    "        myDFTemp = myDFTemp2\n",
    "\n",
    "        #print(\"Replacing commas with semi-colons for CSV output\")\n",
    "        #Commas sometimes appear in rich text fields\n",
    "        #dfCleanedCommas = myDFTemp.applymap(lambda x: x.replace('\"',',') if isinstance(x, str) else x)\n",
    "        #myDFTruncated['eventId'] = myDFTemp['eventId']\n",
    "        if product == 'drugevent' and reportType == 'Analysis':\n",
    "            myDFTruncated = pd.DataFrame(data=myDFTemp)\n",
    "            myDFTruncated['eventAnalysisTruncated'] = myDFTemp['eventAnalysis'].str[:31900] #32877 is known limit\n",
    "\n",
    "            #\"\" replace with \\\"\\\"\n",
    "            myDFTruncated2 = pd.DataFrame(data=myDFTruncated)\n",
    "            #myDFTruncated2Temp = myDFTruncated2\n",
    "            #myDFTruncated2Analysis = calc(myDFTruncated['eventAnalysis'])\n",
    "            #myDFTruncated2Temp['eventAnalysisTruncated'] = cleanDoubleQuotes(myDFTruncated2)\n",
    "            #myDFTruncated2 = myDFTruncated.applymap(lambda x: x.replace('\\\"\"','\\\"\\\\') if isinstance(x, str) else x)\n",
    "            #myDFTruncated['eventAnalysisTruncated2'] = myDFTemp['eventAnalysis'].str[31901:64000]\n",
    "            #myDFTruncated['eventAnalysisTruncated3'] = myDFTemp['eventAnalysis'].str[64001:96000]\n",
    "            myDFTruncated3 = pd.DataFrame(data=myDFTruncated2, columns=['eventAnalysisTruncated'])\n",
    "\n",
    "            #myDFTruncated3Temp = myDFTemp.columns#.drop(index='eventId','264166')\n",
    "            #print(myDFTruncated3Temp)\n",
    "            myDFTruncated4 = myDFTruncated3.rename(columns={\"eventAnalysisTruncated\":\"eventAnalysis\"})\n",
    "            #myDFTruncated3 = myDFTruncated2.rename(columns={\"eventAnalysisTruncated\":\"eventAnalysis\",\"eventAnalysisTruncated2\":\"eventAnalysisContinued\"})\n",
    "\n",
    "            #print(myDFTruncated3)\n",
    "            #myDFTruncated4 = myDFTruncated3.applymap(lambda x: x.replace('\"',',') if isinstance(x, str) else x)\n",
    "            #print(myDFTruncated3)\n",
    "            myDF = myDFTruncated4\n",
    "\n",
    "        elif product == 'drugcatalyst' and reportType == 'Analysis':\n",
    "            #myDFCleanedTemp = myDFTemp.replace(\"'\",\"\")\n",
    "            #print(myDFCleanedTemp)\n",
    "            '''#myDFTruncated = pd.DataFrame(data=myDFTemp)\n",
    "            #myDFTruncated['notes'] = myDFTemp['notes']#.replace('\"','', inplace=True) \n",
    "            #myDFTruncated2 = pd.DataFrame(data=myDFTruncated, columns=['notes'])\n",
    "            #print(myDFTruncated2)'''\n",
    "            myDF = myDFTemp.dropna(how='all')\n",
    "            #myDFCatalystAnalysis = myDFTemp.dropna(how='all')\n",
    "            #print(myDFCatalystAnalysis)\n",
    "            #myDF = myDFCatalystAnalysis\n",
    "        elif product == 'trial' and reportType == 'OutcomeDetails':\n",
    "            myDFTruncated = pd.DataFrame(data=myDFTemp)\n",
    "            myDFTruncated['detailsTruncated'] = myDFTemp['trialOutcomeDetails'].str[:32000]\n",
    "            myDFTruncated2 = pd.DataFrame(data=myDFTruncated, columns=['detailsTruncated'])\n",
    "            myDFTruncated3 = myDFTruncated2.rename(columns={\"detailsTruncated\":\"trialOutcomeDetails\"})\n",
    "            myDF = myDFTruncated2.dropna(how='all')\n",
    "        elif product == 'trial' and reportType == 'Notes':\n",
    "            myDFTruncated = pd.DataFrame(data=myDFTemp)\n",
    "            myDFTruncated['detailsTruncated'] = myDFTemp['details'].str[:32000] #32-bit MS Excel CSV workaround: 32877 is observed limit. 32767 is theoretical\n",
    "            #myDFTruncated['details-pt2'] = myDFTemp['details'].str[32001:64000]\n",
    "\n",
    "            myDFTruncated2 = pd.DataFrame(data=myDFTruncated, columns=['detailsTruncated'])\n",
    "            myDFTruncated3 = myDFTruncated2.rename(columns={\"detailsTruncated\":\"details\"})#,\"details-pt2\":\"detailsPt2\"})\n",
    "\n",
    "            #print(myDFTruncated3)\n",
    "            #myDFTruncated3 = myDFTruncated3.applymap(lambda x: x.replace('\"',',') if isinstance(x, str) else x)\n",
    "            print(myDFTruncated3)\n",
    "            myDF = myDFTruncated3.dropna(how='all')\n",
    "\n",
    "        else:\n",
    "            myDF = pd.DataFrame(data=myDFTemp, columns=colArray)\n",
    "        print(myDF)\n",
    "\n",
    "        dfCleanedTemp = myDF\n",
    "        #print(\"Removing non UTF-8 characters\")\n",
    "        #Non UTF-8 characters can exist within data set. The next line eliminates any upload errors for CSV & SQLITE\n",
    "        dfCleaned = dfCleanedTemp.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "        myDF = dfCleaned\n",
    "\n",
    "        productSet = 'Clinical'\n",
    "        if product == 'drugevent' or product == 'drugcatalyst' or product == 'drugeventprofile' or product == 'drugcatalysttimeseries':\n",
    "            productSet = 'Commercial'\n",
    "\n",
    "        # Load the data based on 'product' and 'date' (\n",
    "        # e.g. 'C:\\\\Users\\\\maddenjo\\\\OneDrive - Informa plc\\\\OneDriveWorkspace\\\\APIOutput\\\\drug20191019\\\\'\n",
    "        inputPath = base_path + pathDelimiter + product + str(date) + pathDelimiter\n",
    "\n",
    "        # Store output with a timestamp\n",
    "        #todayUTCdateTime = datetime.utcnow().strftime(\"%Y%m%d%H%M\")\n",
    "        todayUTCdateTime = datetime.utcnow().strftime(\"%H%M\")\n",
    "\n",
    "        outputRootDirectory = base_path + pathDelimiter + productSet + str(date)  \n",
    "\n",
    "        ### CSV OUTPUT \n",
    "\n",
    "        print(\"Writing to CSV - Do not close this window\")\n",
    "\n",
    "        if os.path.exists(outputRootDirectory) == False:\n",
    "            try:\n",
    "                os.mkdir(outputRootDirectory)\n",
    "            except OSError:\n",
    "                print (\"Creation of the directory %s failed\" % outputRootDirectory)\n",
    "\n",
    "        outputDirectory = outputRootDirectory + pathDelimiter + product + reportType + str(date) #+ str(todayUTCdateTime)\n",
    "        df_csv = myDF.to_csv(path_or_buf=str(outputDirectory)+'.csv')\n",
    "\n",
    "\n",
    "        '''\n",
    "        #outputFileName = base_path + pathDelimiter + 'output' + pathDelimiter + product  \n",
    "        ##df_csv = myDF.to_csv(path_or_buf=str(outputFileName)+reportType+str(date)+str(datetime.utcnow().strftime(\"%H%M\"))+\".csv\", escapechar='\\'')\n",
    "        ##df_csv = myDF.to_csv(path_or_buf=str(outputDirectory)+'.csv', escapechar='\\'')\n",
    "        ##df_csv = myDF.to_csv(path_or_buf=str(outputDirectory)+'.csv', doublequote=False, escapechar='\\'')\n",
    "        ##df_csv = myDF.to_csv(path_or_buf=str(outputDirectory)+'.csv', index=False, quoting=csv.QUOTE_NONE, escapechar='\\'')'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### MS Excel OUTPUT\n",
    "        #excelFile = outputFileName + \".xlsx\"\n",
    "        #print(\"Writing to MS Excel - Do not close this window\")\n",
    "        #with pd.ExcelWriter(excelFile) as writer:  \n",
    "        #\tmyDF.to_excel(writer, sheet_name=\"AllData\")\n",
    "\n",
    "\n",
    "        ### PostgreSQL Output\n",
    "        #print(\"Writing to postgres instance - Do not close this window\")\n",
    "        #engine = create_engine('postgresql://maddenjo:maddenjo@localhost:5432/informaAPIs',echo=False)\n",
    "        #engine = create_engine('postgresql://maddenjo:maddenjo@localhost:5432/Clinical',echo=False)\n",
    "        #engine = create_engine('postgresql://maddenjo:maddenjo@localhost:5432/Commercial',echo=False)\n",
    "        #df_postgres = myDF.to_sql(name=product+str(date)+reportType, con=engine, if_exists = 'replace', index=True)\n",
    "        ###df_postgres = df_toExport.to_sql(name=product+str(date)+reportType, con=engine, if_exists = 'append', index=True)\n",
    "\n",
    "        return None\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        #Define where your source JSON files are located \n",
    "\n",
    "        ###### EDIT THESE NEXT FEW LINES #####\n",
    "\n",
    "        date = ''\t\n",
    "        base_path = 'C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial'\n",
    "        #base_path = 'C:\\\\Users\\\\maddenjo\\\\OneDrive - Informa plc\\\\OneDriveWorkspace\\\\APIOutput'\n",
    "        osType = 'windows' # other options are mac or linux\t\n",
    "        #### End EDIT #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pathDelimiter = str('\\\\')\n",
    "\n",
    "        if osType.lower() != 'windows':\n",
    "            pathDelimiter = str('/')\n",
    "\n",
    "\n",
    "        #Gather the keys to flatten based on 'jsonETLmap.csv'  \n",
    "        inputMetadata = getMetadata()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        inputReports = []\n",
    "        #inputReports = inputMetadata.to_numpy()\n",
    "\n",
    "        #product = \"\"\n",
    "        #product = inputReports[0][0]\n",
    "\n",
    "\n",
    "        #import argparse\n",
    "\n",
    "        #parser = argparse.ArgumentParser()\n",
    "        #parser.add_argument('--reportId', nargs='*', action='append', dest='idList')\n",
    "        #parser.add_argument('--date', nargs='*', action='append', dest='dList')\n",
    "\n",
    "        #args = parser.parse_args()\n",
    "        #print(args)\n",
    "        #product = \"\"\n",
    "\n",
    "        #productList = args.alist\n",
    "        #reportList = args.idList\n",
    "        #dateList = args.dList\n",
    "        try:\n",
    "\n",
    "            #print(productList[0])\n",
    "            #product = productList[0][0]\n",
    "            reportId = reportList[0][0]\n",
    "            print(dateList[0])\n",
    "            date = dateList[0][0]\n",
    "\n",
    "            inputMetadata = getMetadataById(reportId)\n",
    "            inputReports = inputMetadata.to_numpy()\n",
    "\n",
    "\n",
    "            product = inputReports[0][0]\n",
    "            print(product)\n",
    "\n",
    "        except Exception:\n",
    "            print('No args supplied at run-time. We will use value supplied in script')\n",
    "            if len(inputMetadata) == 0:\n",
    "                print('You did not select any reports - please add 1 or more reports via the jsonETLmap.csv file')\n",
    "                print('Program Exiting Now...')\n",
    "                print('')\n",
    "            else:\n",
    "                inputReports = inputMetadata.to_numpy()\n",
    "                product = inputReports[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Load JSON data into memory\n",
    "        path =  base_path + pathDelimiter + str(product) + str(date) + pathDelimiter  \n",
    "\n",
    "        inputData = getJSON(path)\n",
    "\n",
    "        #Initiate variables used later in the process\n",
    "        subKeys = []\n",
    "        subSubKeys = []\n",
    "        queryData = []\n",
    "        objectColumns = []\n",
    "        subKeys = []\n",
    "        subSubKeys = []\n",
    "        colArray = []\n",
    "        reportType = \"\"\n",
    "        primaryKey = getPrimaryKey(product)\n",
    "\n",
    "        # Run each report specified in your map.csv file\n",
    "        for row in inputReports:\n",
    "            if row[0] != product:\n",
    "                product = row[0]\n",
    "                primaryKey = getPrimaryKey(product)\n",
    "                path =  base_path + pathDelimiter + str(product) + str(date) + pathDelimiter\n",
    "                inputData = {}\n",
    "                inputData = getJSON(path)\n",
    "\n",
    "\n",
    "            objectColumns = [row[1]]\n",
    "            queryData = [primaryKey,row[1]]\n",
    "\n",
    "            subKeys = [row[2]]\n",
    "\n",
    "            subSubKeys = [row[3]]\n",
    "\n",
    "\n",
    "            listColsTemp = row[4]\n",
    "            listCols = listColsTemp.split('.')\n",
    "\n",
    "\n",
    "            richTextCols = row[5]\n",
    "\n",
    "            colArrayTemp = row[6]\n",
    "            colArray = colArrayTemp.split('.')\n",
    "\n",
    "            reportType = row[7]\n",
    "\n",
    "\n",
    "\n",
    "            createOutput(product, inputData, queryData, objectColumns, subKeys, subSubKeys,colArray,reportType, base_path,listCols,richTextCols, pathDelimiter, date)\n",
    "\n",
    "    #=====#\n",
    "\n",
    "    #3\n",
    "\n",
    "    import pandas as pd\n",
    "    from pandas.io.json import json_normalize\n",
    "    import numpy as np\n",
    "    import os, json, time\n",
    "    from datetime import date, datetime, timezone\n",
    "    import csv #Used to save as CSV\n",
    "    import openpyxl #MS Excel Writer Package\n",
    "    import sqlite3\n",
    "    from sqlite3 import Error\n",
    "\n",
    "    def read_input(inputListPath):\n",
    "        #Assumes you have a header row in your CSV file\n",
    "        inputList = []\n",
    "        with open(inputListPath, 'r') as csvfile:\n",
    "            numRows = 0\n",
    "            for row in csv.reader(csvfile.read().splitlines()):\n",
    "                if numRows != 0:\n",
    "                    inputList.append(row)\n",
    "                numRows += 1\n",
    "            #print('Processing ' + str(len(inputList)) + ' records now. Do not close this window...')\n",
    "        return inputList\n",
    "\n",
    "    def getPrimaryKey(product):\n",
    "        primaryKey = product + \"Id\"\n",
    "        apiEndpointMap = read_input('./primaryKeyMap.csv')\n",
    "\n",
    "        for item in apiEndpointMap:\n",
    "\n",
    "\n",
    "            if product == item[0]:\n",
    "                primaryKey = item[1] + \"Id\"\n",
    "\n",
    "\n",
    "        return primaryKey\n",
    "\n",
    "    def create_connection(db_file):\n",
    "        \"\"\" create a database connection to the SQLite database\n",
    "            specified by db_file\n",
    "        :param db_file: database file\n",
    "        :return: Connection object or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            c = conn.cursor()\n",
    "            return conn, c\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def getNumFiles(filenames):\n",
    "        numFiles = 0\n",
    "        for file in filenames:\n",
    "            numFiles += 1\n",
    "        return numFiles\n",
    "\n",
    "    def getJSON(path):\n",
    "        '''Loop through each JSON file and add JSON to an array. Pass the array back to main'''\t \n",
    "        filenames = os.listdir(path)\n",
    "        recordHolder = []\n",
    "        totalNumFiles = getNumFiles(filenames)\n",
    "\n",
    "        recordsPerFile = 100\n",
    "\n",
    "        numFiles = 0\n",
    "        print('Loading JSON from ' + str(totalNumFiles) + ' total records - do not close this window...')\n",
    "        for file in filenames:\n",
    "            numFiles += 1\n",
    "            filename = path + file\n",
    "\n",
    "            #Load each JSON file and store into an Array\n",
    "            with open(filename,\"r\") as f:\n",
    "                jsondata=json.load(f)\n",
    "                recordHolder.append(jsondata['items'])\n",
    "\n",
    "                #Optional section - Keep track of records and print output\n",
    "                blockSize = 500\n",
    "                if (numFiles % blockSize == 0):\n",
    "                    print('Processed ' + str(numFiles*100) + ' records from local directory so far. Do not close this window...')\n",
    "\n",
    "            f.close()\n",
    "        print('Data load complete. Processed ' + str(numFiles*100) + ' records from local directory')\n",
    "        return recordHolder\n",
    "\n",
    "\n",
    "    def recursiveKeyLookup(src, dpth = 0, key = '', keys = None, keySet = None):\n",
    "        '''This is a recursive approach to iterating across each JSON key,value pair'''\t \n",
    "        if keys is None:\n",
    "            keys = []\n",
    "\n",
    "        if keySet is None:\n",
    "            keySet = {}\n",
    "\n",
    "        if isinstance(src, dict):\n",
    "\n",
    "            for key, value in src.items():\n",
    "\n",
    "                if not key in keys:\n",
    "                    keys.append(key)\n",
    "\n",
    "                #First, check for dictionary objects\n",
    "                if isinstance(value, dict):\n",
    "\n",
    "                    #Child keys will be created with parentKey\n",
    "                    #Prevent duplicate keys by removing parent key of dictionary items. \n",
    "                    keys.remove(key)\n",
    "\n",
    "                    for k,v in value.items():\n",
    "\n",
    "                        if isinstance(v, list):\n",
    "                            keyColValDict = \"\"\n",
    "                            for x in v:\n",
    "                                keyColValDictTemp = \"\"\n",
    "                                if isinstance(x, dict):\n",
    "\n",
    "                                    for xKey, xVal in x.items():\n",
    "                                        if isinstance(xVal, list):\n",
    "\n",
    "                                            for y in xVal:\n",
    "                                                if isinstance(y, dict):\n",
    "                                                    for yKey, yVal in y.items():\n",
    "                                                        keyColValDictTemp +=  str(yKey).upper() + \":\"+ str(yVal) + \";\"\t\n",
    "                                        else:\n",
    "\n",
    "                                            keyColValDictTemp +=  str(xKey).upper() + \":\"+ str(xVal) + \";\"\n",
    "\n",
    "                                keyColValDict += keyColValDictTemp\n",
    "\n",
    "                            if keyColValDict != \"\":\n",
    "                                v = keyColValDict\n",
    "\n",
    "                        if not k in keys:\n",
    "\n",
    "                            #create new key with original parent key name + new value\n",
    "                            keyCol = str(key).upper()+str(k)\n",
    "                            keys.append(keyCol)\n",
    "                            keySet[keyCol] = v\n",
    "\n",
    "                #Second, check list objects\t\t\t\n",
    "                elif isinstance(value, list):\n",
    "\n",
    "                    keyColValTempMain2 = \"\"\n",
    "\n",
    "                    for val in value:\n",
    "\n",
    "                        keyColValTemp3 = \"\"\n",
    "                        #Check for dictionary objects within the list\n",
    "                        if isinstance(val, dict):\n",
    "\n",
    "                            if value in keys:\n",
    "                                keys.remove(value)\n",
    "                            keyCol = \"\"\n",
    "                            #for kar in val:\n",
    "                            #\tif isinstance(k, list):\n",
    "                                #print(str(kar) + \" | \" + str(type(kar)))\n",
    "\n",
    "                            keyColTemp2 = \"\"\n",
    "                            for k,v in val.items():\n",
    "                                #Traverse list items found within dictionaries\n",
    "\n",
    "                                if isinstance(v, list): \n",
    "\n",
    "                                    keyColValTempMain = \"\"\n",
    "                                    keyColValTempKeeper = \"\"\n",
    "                                    for children in v:\n",
    "                                        keyColValTemp = \"\"\n",
    "\n",
    "                                        if isinstance(children, dict):\n",
    "\n",
    "                                            for parentChild, child in children.items():\n",
    "                                                #Use semi-colons to delimt the flattened string\n",
    "                                                #Some keysets do not have an ID, so we'll keep the \"JSONish\" tags \n",
    "\n",
    "                                                if isinstance(child, list):\n",
    "                                                    for chil in child:\n",
    "                                                        #Confirm that we have a dictionary item\n",
    "                                                        if isinstance(chil, dict):\n",
    "                                                            for chilKey, chilVal in chil.items():\n",
    "                                                                # Creating the json-ish tag: Ex/ \"id:12134;name:Smith\"\n",
    "                                                                keyColValTemp += str(k).lower() + str(chilKey).upper() + \":\"+ str(chilVal) + \";\"\n",
    "                                                        else:\n",
    "                                                            # A few structures have combinations of lists with dictionaries and lists with strings.\n",
    "                                                            # When we encounter a non-dictionary object, simply concatenate each value\n",
    "                                                            keyColValTemp += str(k).lower() + \":\" + str(chil) + \";\"\n",
    "                                                else:\n",
    "                                                    # Although more dicitonary items could live here, let's concatenate everything from this point on\n",
    "                                                    keyColValTemp += str(k).lower() + str(parentChild).upper() + \":\"+ str(child) + \";\"\n",
    "\n",
    "                                        else:\n",
    "                                            # Concatenativng non dictionary items within 'children'\n",
    "                                            keyColValTemp += str(children) + \";\"\n",
    "\n",
    "                                        # Storing 'keyColValTemp' into another string while we continue looping\n",
    "                                        keyColValTempKeeper += keyColValTemp\n",
    "\n",
    "                                    # Storing keyColValTempKeeper into one more temp string to wrap up the loops\n",
    "                                    keyColValTempMain += keyColValTempKeeper\n",
    "\n",
    "                                    #Save the flattened dictionary items found within lists\n",
    "                                    if keyColValTempMain != \"\":\n",
    "                                        keyColValTempMain2 += keyColValTempMain\n",
    "\n",
    "\n",
    "                                else:\n",
    "                                    #If the key already exists, apend to it rather than creating a new column\n",
    "                                    if key in keys:\n",
    "                                        keyColTemp2 += str(k) + \":\" + str(v) + \"; \"\n",
    "\n",
    "                            # Concatenate each temp variable as we loop to next key\n",
    "                            keyColValTempMain2 += keyColTemp2\n",
    "\n",
    "                            #Confirm that we are storing in a non-null value\n",
    "                            if keyColValTempMain2 != \"\":\n",
    "                                keySet[key] = keyColValTempMain2\n",
    "\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            #We confirmed no more dictionary objects are in the list.\n",
    "                            #Take the remaining list elements and simply concatenate as a string\n",
    "                            i = 0\n",
    "                            keyValString = \"\"\n",
    "                            while i < len(value):\n",
    "                                tmpVal = value[i]\n",
    "                                if i == len(value) - 1:\n",
    "                                    keyValString += str(tmpVal)\n",
    "                                else:\n",
    "                                    keyValString += str(tmpVal) + \", \"\n",
    "                                i += 1\n",
    "                            keySet[key] = keyValString\n",
    "\n",
    "                #Lastly, get the objects that sit at the top of the JSON hierarchy and do not contain dictionary/list objects\t\t\t\t\t \n",
    "                elif not isinstance(value, list) and not isinstance(value, dict):\n",
    "                    if key in keys and dpth==0:\n",
    "                        keySet[key] = value\n",
    "\n",
    "                #Pass found keys back into recursive method\n",
    "                recursiveKeyLookup(value, dpth + 1, key, keys, keySet)\n",
    "\n",
    "        return keySet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compressJSON(srcArray):\n",
    "\n",
    "\n",
    "        #JSON currently is stored within many arrays. The next lines pull json up a level\n",
    "        srcJSONs = []\n",
    "\n",
    "        for src in srcArray:\n",
    "            for s in src:\n",
    "                srcJSONs.append(s)\n",
    "\n",
    "        #Optimize the runtime of this script by setting the block size \n",
    "        #Rather than processing all records at once, break into blocks\n",
    "        print(\"Number of Records: \" + str(len(srcJSONs)))\n",
    "        numSrcJSONs = len(srcJSONs)\n",
    "        blockSize = 1000\n",
    "        print(\"Block Size = \" + str(blockSize))\n",
    "        if len(srcJSONs) < blockSize:\n",
    "            blockSize=len(srcJSONs)\n",
    "\n",
    "        #Esnure the number of loops matches the record set\n",
    "        numBlocks = int(len(srcJSONs)/blockSize)\n",
    "        #The final block will be smaller than the other blocks. \n",
    "        diff = len(srcJSONs) - (numBlocks * blockSize)\n",
    "\n",
    "        #Scan the array and put the final block into a separate array\n",
    "        lenSRCJSON = len(srcJSONs)\n",
    "        lastTotalBlockSize = diff\n",
    "        lastTotalBlock = srcJSONs[lenSRCJSON - diff:]\n",
    "\n",
    "        #If you select a blocksize that results in 0 blocks, we ensure the looping picks up every record\n",
    "        if numBlocks==0:\n",
    "            blockSize=len(srcJSONs)\n",
    "            lastTotalBlock=[srcJSONs]\n",
    "        print(\"Last Total Block Size = \" + str(lastTotalBlockSize))\n",
    "\n",
    "        print('Attempting to process : ' + str(numBlocks) + \" blocks of \" + str(blockSize) +\" records\")\n",
    "\n",
    "        flatJSONs = []\n",
    "        blockNum = 0\n",
    "        while blockNum < numBlocks:\n",
    "            srcJSONBlock = []\n",
    "            x = 0\n",
    "            #Using the blocksize specified above, jump to the next set of records and store into a temporary array\n",
    "            while x < blockSize:\n",
    "                srcJSONBlock.append(srcJSONs[(blockNum*blockSize)+x])\n",
    "                x += 1\n",
    "\n",
    "            #Leveraging the array above, flatten each array element via \"recursiveKeyLookup\" function\n",
    "            y = 0\n",
    "            while y < len(srcJSONBlock):\n",
    "                flatJSON = recursiveKeyLookup(srcJSONBlock[y])\n",
    "                flatJSONs.append(flatJSON)\n",
    "                y += 1\n",
    "\n",
    "            #Optional progress checker\n",
    "            if (blockNum % 1000) == 0:\n",
    "                print('Flattened ' + str(blockNum) + ' records. Do not close this window...')\t\t\n",
    "\n",
    "            blockNum += 1\n",
    "\n",
    "        print(\"Length of flatJSONs before extra sets added = \" + str(len(flatJSONs)))\n",
    "\n",
    "        #Add the remaining values\n",
    "        k = 0\n",
    "        while k < len(lastTotalBlock):\n",
    "            moreUnFlatJSON = lastTotalBlock[k]\n",
    "            moreFlatJSON = recursiveKeyLookup(moreUnFlatJSON)\n",
    "            flatJSONs.append(moreFlatJSON)\n",
    "            k += 1\n",
    "\n",
    "        print(\"Done processing blocks\")\n",
    "        print(\"Length of flatJSONs after extra sets added = \" + str(len(flatJSONs)))\n",
    "\n",
    "\n",
    "        return flatJSONs\n",
    "\n",
    "\n",
    "    def cleanupDF(df, primaryKey):\n",
    "        #print(df.columns)\n",
    "        dfTemp = df\n",
    "        if product == 'drugevent':\n",
    "            df = dfTemp.drop(columns=['eventAnalysis'])\n",
    "        elif product == 'drugcatalyst':\n",
    "            df = dfTemp.drop(columns=['catalystAnalysis'])\n",
    "        elif product == 'trial':\n",
    "            df = dfTemp.drop(columns=['trialNotes','trialResults','trialOutcomeDetails'])\n",
    "\n",
    "        #print(\"Replacing commas with semi-colons for CSV output\")\n",
    "        #Commas sometimes appear in rich text fields\n",
    "        #dfCleanedCommas = df.applymap(lambda x: x.replace(';',',') if isinstance(x, str) else x)\n",
    "\n",
    "        print(\"Removing non UTF-8 characters\")\n",
    "        #Non UTF-8 characters can exist within data set. The next line eliminates any upload errors for CSV & SQLITE\n",
    "        dfCleaned = df.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "\n",
    "        print(\"Setting an Index for the dataframe\")\n",
    "        #Set the pandas index \n",
    "\n",
    "        df_wIndex = dfCleaned.set_index(primaryKey)\n",
    "\n",
    "        print(\"sorting columns to appear in alphabetical order\")\n",
    "        #Sort the columns in alphabetical order\n",
    "        df_sorted = df_wIndex.sort_values(by=primaryKey)\n",
    "\n",
    "        print(df_sorted.columns)\n",
    "        #df_toExportTemp = df_sorted.drop(columns=['copyrightNotice','$type'])\n",
    "        df_toExportTemp = df_sorted.drop(columns=['$type'])\n",
    "\n",
    "        #df_toExportTemp2 = df_toExportTemp.rename(str.lower, axis='columns')\n",
    "\n",
    "        #Set variable that will be leveraged by various export functions\n",
    "        df_toExport = df_toExportTemp\n",
    "\n",
    "        return df_toExport\n",
    "\n",
    "    def cleanDoubleQuotes(value):\n",
    "        print(type(value))\n",
    "\n",
    "        valueCleaned = value.applymap(lambda x: x.replace('\"','') if isinstance(x, str) else x)\n",
    "        print(valueCleaned)\n",
    "        #return value.replace('\"\"', '')\n",
    "        return valueCleaned\t\t\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        #Define where your source JSON files are located \n",
    "\n",
    "        ###### EDIT THESE NEXT FEW LINES #####\n",
    "        product = \"trial\"  \t# Alternative choices: \"trial\", \"investigator\", \"organization\", \"drugevent\", \"drugeventprofile\",  \"drugcatalyst\", \"deviceproduct\"\n",
    "        date = datetime.now().strftime(\"%Y%m%d\") \n",
    "        base_path = 'C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial'\n",
    "        Clinical = \"Clinical\"\n",
    "        osType = 'windows' # other options are mac or linux\t\n",
    "        #### End EDIT #####\n",
    "\n",
    "        pathDelimiter = str('\\\\')\n",
    "\n",
    "        if osType.lower() != 'windows':\n",
    "            pathDelimiter = str('/')\n",
    "\n",
    "        #import argparse\n",
    "\n",
    "        #parser = argparse.ArgumentParser()\n",
    "        #parser.add_argument('--product', nargs='*', action='append', dest='alist')\n",
    "        #parser.add_argument('--date', nargs='*', action='append', dest='dList')\n",
    "\n",
    "        #args = parser.parse_args()\n",
    "        #print(args)\n",
    "        #print(args.split())\n",
    "        #testObject = args.split()\n",
    "        #productList = args.alist\n",
    "        #dateList = args.dList\n",
    "        try:\n",
    "        #if productList != [] or productList != [[]]:\n",
    "            print(productList[0])\n",
    "            product = productList[0][0]\n",
    "            print(dateList[0])\n",
    "            date = dateList[0][0]\n",
    "        except Exception:\n",
    "            print('No args supplied at run-time. We will use value supplied in script')\n",
    "\n",
    "        productSet = 'Clinical'\n",
    "        if product == 'drugevent' or product == 'drugcatalyst' or product == 'drugeventprofile' or product == 'drugcatalysttimeseries':\n",
    "            productSet = 'Commercial'\n",
    "\n",
    "        # Load the data based on 'product' and 'date' (\n",
    "        # e.g. 'C:\\\\Users\\\\maddenjo\\\\OneDrive - Informa plc\\\\OneDriveWorkspace\\\\APIOutput\\\\drug20191019\\\\'\n",
    "        inputPath = base_path + pathDelimiter + product + pathDelimiter\n",
    "\n",
    "        # Store output with a timestamp\n",
    "        todayUTCdateTime = datetime.utcnow().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "\n",
    "\n",
    "        #Load JSON from your local folder\n",
    "        srcArray = getJSON(inputPath)\n",
    "\n",
    "        #Flatten JSON - 1 row for each record\n",
    "        compressedJSONs = compressJSON(srcArray)\n",
    "\n",
    "        ######SAVE FLATTENED JSON TO YOUR PREFERRED OUTPUT FORMAT (.csv, .xlsx, .sql) \n",
    "        df = pd.DataFrame(data=compressedJSONs)\n",
    "\n",
    "        print(\"processed dataframe\")\n",
    "        primaryKey = getPrimaryKey(product)\n",
    "        print(df.columns)\n",
    "\n",
    "\n",
    "        df_toExportTemp = cleanupDF(df, primaryKey)\n",
    "        print(\"Cleaned up dataframe. Ready to store in preferred format(s)\")\n",
    "\n",
    "        print(\"Removing doublequotes\")\n",
    "        df_toExport = cleanDoubleQuotes(df_toExportTemp)\n",
    "\n",
    "\n",
    "\n",
    "        ### CSV OUTPUT \n",
    "        outputRootDirectory = base_path + pathDelimiter + pathDelimiter + product \n",
    "\n",
    "\n",
    "        if os.path.exists(outputRootDirectory) == False:\n",
    "            try:\n",
    "                os.mkdir(outputRootDirectory)\n",
    "            except OSError:\n",
    "                print (\"Creation of the directory %s failed\" % outputRootDirectory)\n",
    "\n",
    "        outputDirectory = outputRootDirectory + pathDelimiter + product  + str(date) #+ todayUTCdateTime\t\n",
    "\n",
    "\n",
    "        print(\"Writing to CSV - Do not close this window\")\n",
    "        '''##df_csv = df_toExport.to_csv(path_or_buf=str(outputDirectory)+'.csv', escapechar='\\'')'''\n",
    "        df_csv = df_toExport.to_csv(path_or_buf=str(outputDirectory)+'.csv')\n",
    "\n",
    "\n",
    "\n",
    "        ### MS Excel OUTPUT\n",
    "        #excelFile = outputDirectory + \".xlsx\"\n",
    "        #print(\"Writing to MS Excel - Do not close this window\")\n",
    "        #with pd.ExcelWriter(excelFile) as writer:  \n",
    "            #df_toExport.to_excel(writer, sheet_name=\"AllData\")\n",
    "\n",
    "        ### SQL OUTPUT \n",
    "        #database = 'C:\\\\Users\\\\username\\\\Toolkits\\\\sqlite\\\\sqliteDBname.sql'\n",
    "        # create a database connection\n",
    "        #conn, c = create_connection(database)\n",
    "        #myCursor = conn.cursor\n",
    "        #df_sql = df_toExport.to_sql(str(product)+str(todayUTCdateTime),conn)\n",
    "\n",
    "        ### PostgreSQL Output\n",
    "        #from sqlalchemy import create_engine \n",
    "        ##from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "        #engine = create_engine('postgresql://maddenjo:maddenjo@localhost:5432/informaAPIs',echo=False)\n",
    "        #engine = create_engine('postgresql://maddenjo:maddenjo@localhost:5432/Clinical',echo=False)\n",
    "        #df_postgres = df_toExport.to_sql(name=product+str(date), con=engine, if_exists = 'replace', index=True)\n",
    "        ##df_postgres = df_toExport.to_sql(name=product+str(date), con=engine, if_exists = 'append', index=True)\n",
    "\n",
    "        print(\"Done\")\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import date, datetime, timedelta\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    ttdata = pd.read_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\trial\\\\trial\"+datetime.now().strftime(\"%Y%m%d\")+\".csv\")\n",
    "\n",
    "    print(ttdata.shape)\n",
    "    ttdata.head()\n",
    "\n",
    "    ttpatientsegment = pd.read_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\Clinical\\\\trialPatientSegments.csv\")\n",
    "\n",
    "    print(ttpatientsegment.shape)\n",
    "    ttpatientsegment.head()\n",
    "\n",
    "    tdata = pd.merge(ttdata, ttpatientsegment[[\"trialId\", \"trialPatientSegments\"]], on='trialId').drop_duplicates([\"trialId\"])\n",
    "    print(tdata.shape)\n",
    "    tdata.head()\n",
    "\n",
    "    tdata.columns\n",
    "\n",
    "    tt_ct = tdata[['trialId', 'trialCountries', 'ctGovListedLocations', 'trialObjective', 'trialInclusionCriteria',\n",
    "                   'trialExclusionCriteria', 'trialTreatmentPlan', 'trialProtocolIDs', 'trialSupportingUrls', 'trialTags', \n",
    "                   'trialOutcomes', 'trialTargetAccrual', 'trialPatientSegments', 'trialPrimaryEndpointsReported']]\n",
    "\n",
    "    ### Print shape ###\n",
    "    print(tt_ct.shape)\n",
    "\n",
    "    tt_ct\n",
    "\n",
    "    df = pd.DataFrame(tt_ct)\n",
    "\n",
    "    df.to_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\tt_ct\"+datetime.now().strftime(\"%Y%m%d\")+\".csv\")\n",
    "    #import os\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\Clinical\\\\trialInvestigators.csv')\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\Clinical\\\\trialNotes.csv')\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\Clinical\\\\trialPatientSegments.csv')\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\Clinical\\\\trialSponsors.csv')\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\Clinical\\\\trialtrialPrimaryDrugsTested.csv')\n",
    "    \n",
    "    #4\n",
    "    import shutil\n",
    "   # shutil.rmtree('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\trial')\n",
    "    #os.remove('C:\\\\Users\\\\11201312\\Desktop\\output2\\\\trial\\\\tt_ct20200227.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    RA()\n",
    "    \n",
    "#import schedule\n",
    "#import time, os\n",
    "#import psutil\n",
    "#schedule.every(.10).minutes.do(RA)\n",
    "#while True:\n",
    " #   schedule.run_pending()\n",
    "  #  time.sleep(.10)\n",
    "\n",
    "\n",
    "def RA():\n",
    "    ## Importing Required packeges\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import date, datetime, timedelta\n",
    "    import time\n",
    "    import datacompy\n",
    "    import os\n",
    "    from pyspark.sql import Row\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By \n",
    "    from selenium.webdriver.support.ui import WebDriverWait \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    #### Open browser\n",
    "    browser = webdriver.Chrome(executable_path='C:\\\\Users\\\\11201312\\\\Downloads\\\\chromedriver_win32 (2)\\\\chromedriver.exe')\n",
    "    browser.get(\"https://clinicaltrials.gov/ct2/results?cond=COVID-19\")\n",
    "    \n",
    "    ### Dowloading data from browser\n",
    "    browser.find_element_by_xpath('/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[1]/div[3]/a').click()\n",
    "    browser.find_element_by_xpath('/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[1]/div[4]/div/div/div/form[1]/div[1]/div[2]/select/option[3]').click()\n",
    "    browser.find_element_by_xpath('/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[1]/div[4]/div/div/div/form[1]/div[3]/div[2]/select/option[2]').click()\n",
    "    browser.find_element_by_xpath('/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[1]/div[4]/div/div/div/form[1]/div[4]/div[2]/select/option[4]').click()\n",
    "    browser.find_element_by_xpath('/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[1]/div[4]/div/div/div/form[1]/div[5]/input').click()                               \n",
    "    \n",
    "    time.sleep(20)\n",
    "    browser.close()\n",
    "    ## Read data from downloads\n",
    "    df = pd.read_csv('C:/Users/11201312/Downloads/SearchResults.csv')\n",
    "    print(\"Shape of downloaded data: \", df.shape)\n",
    "    \n",
    "    df.head()\n",
    "    \n",
    "    ## Import NCT ids alog with its phase shared by KC team\n",
    "    kc = pd.read_excel('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/RA__KC_Trials.xlsx')\n",
    "    print(\"Shape of NCT, Phase data Share by KC: \", kc.shape)\n",
    "    \n",
    "    ######################################################################\n",
    "    #### Reading TrialTrove full data #####\n",
    "    ttdata = pd.read_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\trial\\\\trial\"+datetime.now().strftime(\"%Y%m%d\")+\".csv\")\n",
    "    print(\"Shape of vlookup \"+str(ttdata.shape))\n",
    "    #ttdata.head()\n",
    "    \n",
    "    ################ Read Trial PatientSegment Data #####################\n",
    "    ttpatientsegment = pd.read_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\Clinical\\\\trialPatientSegments.csv\")\n",
    "    print(\"Shape of trialPatientSegments \", ttpatientsegment.shape)\n",
    "    \n",
    "    ## Grouping by trialid\n",
    "    ttpatientsegment1 = ttpatientsegment.groupby('trialId', as_index=False).agg(lambda x: x.tolist())\n",
    "    \n",
    "    ## Coverting list columns to string\n",
    "    ttpatientsegment1[\"trialPatientSegments1\"] = ttpatientsegment1['trialPatientSegments'].astype(str).str.replace('\\[|\\]|\\'', '') \n",
    "    print(\"Shape of trialPatientSegments1 \", ttpatientsegment1.shape)\n",
    "\n",
    "    ## Export grouped/modified data to the location\n",
    "    #ttpatientsegment1.to_csv(\"C:\\\\Users\\\\11201116\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project\\\\output2\\\\Clinical\\\\trialPatientSegments1.csv\")\n",
    "    \n",
    "    ### Apply \"Vlookup\" in padas for trialTrove data and PatientSegment1 data\n",
    "    tpdata = pd.merge(ttdata, ttpatientsegment1[[\"trialId\", \"trialPatientSegments1\"]], on='trialId')#.drop_duplicates([\"trialId\"])\n",
    "    print(\"Vlookup TT, PatientSegment data: \" + str(tpdata.shape))\n",
    "    #tdata.head()\n",
    "\n",
    "    ################# Read trialtrialPrimaryDrugsTested data #########################\n",
    "    drugdata = pd.read_csv(\"C:\\\\Users\\\\11201312\\\\Desktop\\\\output2\\\\trial\\\\Clinical\\\\trialtrialPrimaryDrugsTested.csv\")\n",
    "    print(\"Shape of drugdata: \", drugdata.shape)\n",
    "    \n",
    "    ## Grouping by trialid\n",
    "    drugdata1 = drugdata.groupby('trialId', as_index=False).agg(lambda x: x.tolist())\n",
    "        \n",
    "    ## Coverting list columns to string\n",
    "    drugdata1[\"drugName1\"] = drugdata1['drugName'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "    drugdata1[\"drugPrimaryName1\"] = drugdata1['drugPrimaryName'].astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "\n",
    "    print(\"Shape of drugdata1: \",drugdata1.shape)\n",
    "\n",
    "    ## Export grouped/modified data to the location\n",
    "    #drugdata1.to_csv(\"C:\\\\Users\\\\11201116\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project\\\\output2\\\\Clinical\\\\trialtrialPrimaryDrugsTested1.csv\")\n",
    "\n",
    "    ### Apply \"Vlookup\" in padas for tpdata and drugdata1\n",
    "    tpddata = pd.merge(tpdata, drugdata1[[\"trialId\", \"drugName1\"]], on='trialId')#.drop_duplicates([\"trialId\"])\n",
    "    print(\"Vlookup of TT PatientSegment & Drug data shape: \" + str(tpddata.shape))\n",
    "    #tdata.head()\n",
    "\n",
    "    ### Keep Required columns and store it in a dataframe\n",
    "    tt_ct = tpddata[['trialId', 'trialCountries', 'ctGovListedLocations', 'trialObjective', 'trialInclusionCriteria', 'trialExclusionCriteria', \n",
    "               'trialTreatmentPlan', 'trialProtocolIDs', 'trialSupportingUrls', 'trialTags', 'trialOutcomes', 'trialTargetAccrual', \n",
    "               'trialPatientSegments1', 'trialPrimaryEndpointsReported','drugName1']]\n",
    "    print(\"tt_ct shape: \", tt_ct.shape)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "      \n",
    "    ### Integrate TrialTrove code to get required fields to add it to CT data ###\n",
    "    ###### Apply this Vlookup to final aggrigated TT data ######\n",
    "    \n",
    "    ### Apply this Vlookup to final aggrigated TT data\n",
    "    tt1 = tt_ct.merge(kc, on='trialId', how='right')\n",
    "    print(\"Shape of vlookup dataframe: \"+ str(tt1.shape))\n",
    "    tt1.head()\n",
    "    \n",
    "    # apply \"Vlookup\" in padas to Merge/Vlookup TT data to CT data\n",
    "    Updated1 = df.merge(tt1, on='NCT Number')\n",
    "    print(Updated1.shape)\n",
    "    Updated1.head()\n",
    "    \n",
    "    ### Create a DataFrame to get required data fields ###\n",
    "    Data = pd.DataFrame()\n",
    "    \n",
    "    Data['Clinical Trial Identifier'] = Updated1['NCT Number']\n",
    "    Data['Trial Title'] = Updated1['Title']\n",
    "    Data['Title Acronym'] = Updated1['Acronym']\n",
    "    Data['Competitor Asset'] = Updated1['Interventions']\n",
    "    Data['Condition/Disease'] = Updated1['Conditions']\n",
    "    Data['Trial Phase'] = Updated1['trialPhase']\n",
    "    Data['Trial Status'] = Updated1['Status']\n",
    "    Data['Sponsor(s)'] = Updated1['Sponsor/Collaborators']\n",
    "    Data['Target Accrual'] = Updated1['trialTargetAccrual']\n",
    "    Data['Actual Accrual'] = Updated1['Enrollment']\n",
    "    Data['Countries'] = Updated1['trialCountries']\n",
    "    Data['No.of Countries'] = Updated1.trialCountries.str.count(',')+1  # Count no.of countries by counting comma, b/s countries are seperated with comma\n",
    "    Data['Geographical Reach'] = np.where(Data['No.of Countries'] == 1, Data.Countries, \n",
    "                                          np.where(Data['No.of Countries'] > 1, 'Global', 'Null'))  # Country/Global categorization\n",
    "    Data['Study Start Date'] = Updated1['Start Date']\n",
    "    Data['Primary Completion Date'] = Updated1['Primary Completion Date']\n",
    "    Data['Study Completion Date'] = Updated1['Completion Date']\n",
    "    Data['Trial Objective(s)'] = Updated1['trialObjective']\n",
    "    Data['Outcome Measures'] = Updated1['Outcome Measures']\n",
    "    Data['Primary Endpoints Reported Date'] = Updated1['trialPrimaryEndpointsReported']\n",
    "    #Data['Other EndPoint'] = Updated1['Outcome Measures']\n",
    "    Data['Inclusion Criteria'] = Updated1['trialInclusionCriteria']\n",
    "    Data['Exclusion Criteria'] = Updated1['trialExclusionCriteria']\n",
    "    Data['Study Design'] = Updated1['Study Designs']\n",
    "    Data['Patient Segments'] = Updated1['trialPatientSegments1']\n",
    "    Data['Treatment Plan'] = Updated1['trialTreatmentPlan']\n",
    "    Data['Dosing/Comparator'] = Updated1['trialTreatmentPlan']\n",
    "    Data['Age'] = Updated1['Age']\n",
    "    Data['Gender'] = Updated1['Gender']\n",
    "    Data['ROA'] = Updated1['drugName1']\n",
    "    Data['Trial Tag/Attribute'] = Updated1['trialTags']\n",
    "    Data['Trial Outcome(s)'] = Updated1['trialOutcomes']\n",
    "    Data['Other Trial Identifier(s)'] = Updated1['trialProtocolIDs']\n",
    "    Data['Additional Source'] = Updated1['trialSupportingUrls']\n",
    "    Data['Last CT.gov Update noted on'] = Updated1['Last Update Posted']\n",
    "    Data['TrialtroveId'] = Updated1['trialId']\n",
    "    Data['Date'] = str(date.today())\n",
    "    \n",
    "    ### Saving today updated data in local\n",
    "    Data.to_excel('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra_Updated'+str(date.today())+'.xlsx', index=False)\n",
    "    \n",
    "    ### Sape of Latest file\n",
    "    print(\"Shape of latest CT dataframe: \", Data.shape)\n",
    "    \n",
    "    #### Print latest file columns\n",
    "    Data.columns\n",
    "    \n",
    "    ### Subset latest updated data for comparision based on preority \n",
    "    data_comp = Data[['Clinical Trial Identifier', 'Competitor Asset', 'Sponsor(s)', 'Trial Title', 'Condition/Disease', \n",
    "                  'Trial Phase', 'Trial Status', 'Other Trial Identifier(s)', 'Target Accrual', 'Actual Accrual', \n",
    "                  'Title Acronym', 'Countries', 'Trial Objective(s)', 'Study Design', 'Patient Segments', 'Treatment Plan', \n",
    "                  'Dosing/Comparator', 'ROA', 'Outcome Measures', 'Study Start Date', 'Primary Completion Date', \n",
    "                  'Study Completion Date', 'Inclusion Criteria', 'Exclusion Criteria', 'Trial Tag/Attribute', \n",
    "                  'Other Trial Identifier(s)', 'Trial Outcome(s)']]\n",
    "    \n",
    "    ## Shape of comparision data\n",
    "    print(\"Shape of Comparision dataframe: \"+str(data_comp.shape))\n",
    "    \n",
    "    ### Exporting Comparision sheet to local repository \n",
    "    data_comp.to_excel('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra'+str(date.today())+'.xlsx', index=False)\n",
    "                        \n",
    "    #Remove Search resulta data from downloads. Run this line of code after completing regular comparision task.\n",
    "    import os\n",
    "    os.remove('C:/Users/11201312/Downloads/SearchResults.csv')\n",
    "    \n",
    "    ########################################################################################################################\n",
    "    ##### CT Data to display ##### \n",
    "    Data['Registry'] = \"ClinicalTrialgov\"\n",
    "    Data['IDs'] = Data['Clinical Trial Identifier']\n",
    "    \n",
    "    Data_Display_CT = Data[['Clinical Trial Identifier', 'Registry', 'IDs', 'Trial Title', 'Title Acronym', 'Competitor Asset', \n",
    "                            'Condition/Disease', 'Trial Phase', 'Trial Status', 'Sponsor(s)', 'Target Accrual', 'Actual Accrual',\n",
    "                            'Countries', 'Study Start Date', 'Primary Completion Date', 'Study Completion Date', \n",
    "                            'Trial Objective(s)', 'Outcome Measures', 'Primary Endpoints Reported Date', 'Inclusion Criteria', \n",
    "                            'Exclusion Criteria', 'Study Design', 'Patient Segments', 'Treatment Plan', 'Dosing/Comparator', \n",
    "                            'Age', 'Gender', 'ROA', 'Trial Tag/Attribute', 'Trial Outcome(s)', 'Other Trial Identifier(s)', \n",
    "                            'Additional Source', 'Last CT.gov Update noted on']]\n",
    "\n",
    "    print(\"CT Data to display: \", Data_Display_CT.shape)\n",
    "    \n",
    "    ##### TT Data to dispaly #####\n",
    "    ### Apply \"Vlookup\" in padas for tpddata and Updated1\n",
    "    tpddata1 = pd.merge(tpddata, Updated1[[\"trialId\", \"NCT Number\"]], on='trialId')\n",
    "    print(\"Vlookup tpddata and Updated1: \" + str(tpddata1.shape))\n",
    "    #tdata.head()\n",
    "    \n",
    "    ## Dataframe for TT data ##\n",
    "    Data_Display_TT = pd.DataFrame() \n",
    "    \n",
    "    Data_Display_TT['Clinical Trial Identifier'] = tpddata1['NCT Number']\n",
    "    Data_Display_TT['Registry'] = 'Trialtrove'\n",
    "    Data_Display_TT['IDs'] = tpddata1['trialId']\n",
    "    Data_Display_TT['Trial Title'] = tpddata1['trialTitle']\n",
    "    Data_Display_TT['Title Acronym'] = Data_Display_CT['Title Acronym']\n",
    "    Data_Display_TT['Competitor Asset'] = tpddata1['drugName1']\n",
    "    Data_Display_TT['Condition/Disease'] = Data_Display_CT['Condition/Disease']\n",
    "    Data_Display_TT['Trial Phase'] = tpddata1['trialPhase']\n",
    "    Data_Display_TT['Trial Status'] = tpddata1['trialStatus']\n",
    "    Data_Display_TT['Sponsor(s)'] = tpddata1['trialSponsors']\n",
    "    Data_Display_TT['Target Accrual'] = tpddata1['trialTargetAccrual']\n",
    "    Data_Display_TT['Actual Accrual'] = tpddata1['trialActualAccrual']\n",
    "    Data_Display_TT['Countries'] = tpddata1['trialCountries']\n",
    "    Data_Display_TT['Study Start Date'] = tpddata1['trialStartDate']\n",
    "    Data_Display_TT['Primary Completion Date'] = tpddata1['trialPrimaryCompletionDate']\n",
    "    Data_Display_TT['Study Completion Date'] = tpddata1['trialPrimaryCompletionDate']\n",
    "    Data_Display_TT['Trial Objective(s)'] = tpddata1['trialObjective']\n",
    "    Data_Display_TT['Outcome Measures'] = tpddata1['TRIALPRIMARYENDPOINTdetails']\n",
    "    Data_Display_TT['Primary Endpoints Reported Date'] = tpddata1['trialPrimaryEndpointsReported']\n",
    "    Data_Display_TT['Inclusion Criteria'] = tpddata1['trialInclusionCriteria']\n",
    "    Data_Display_TT['Exclusion Criteria'] = tpddata1['trialExclusionCriteria']\n",
    "    Data_Display_TT['Study Design'] = tpddata1['trialStudyDesign']\n",
    "    Data_Display_TT['Patient Segments'] = tpddata1['trialPatientSegments1']\n",
    "    Data_Display_TT['Treatment Plan'] = tpddata1['trialTreatmentPlan']\n",
    "    Data_Display_TT['Dosing/Comparator'] = tpddata1['trialTreatmentPlan']\n",
    "    Data_Display_TT['Age'] = tpddata1['TRIALPATIENTCRITERIAminAge']\n",
    "    Data_Display_TT['Gender'] = tpddata1['TRIALPATIENTCRITERIAgender']\n",
    "    Data_Display_TT['ROA'] = tpddata1['drugName1']\n",
    "    Data_Display_TT['Trial Tag/Attribute'] = tpddata1['trialTags']\n",
    "    Data_Display_TT['Trial Outcome(s)'] = tpddata1['trialOutcomes']\n",
    "    Data_Display_TT['Other Trial Identifier(s)'] = tpddata1['trialProtocolIDs']\n",
    "    Data_Display_TT['Additional Source'] = tpddata1['trialSupportingUrls']\n",
    "    Data_Display_TT['Last CT.gov Update noted on'] = tpddata1['trialLastModifiedDate']\n",
    "\n",
    "    print(\"TT data to dispaly: \", Data_Display_TT.shape)\n",
    "    \n",
    "    ########################### Appending CT and TT data to display it in CI dashboard ###########################\n",
    "    frames = [Data_Display_CT, Data_Display_TT]  ### Createing a frame to append CT & TT trials \n",
    "    CT_TT_Display = pd.concat(frames)  ### Concatenate all dataframes in frames\n",
    "    print(\"History df shape: \", CT_TT_Display.shape)\n",
    "    \n",
    "    CT_TT_Display.to_excel('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra_display.xlsx', index=False)\n",
    "        \n",
    "    ######################################################################################################################## \n",
    "    ######################################## Comparision of two dataframes #################################################\n",
    "    ########################################################################################################################\n",
    "    df1 = pd.read_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra.xlsx\")\n",
    "    df2 = pd.read_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra\"+str(date.today())+\".xlsx\")\n",
    "    \n",
    "    print(\"Shape of Dataframe1: \"+str(df1.shape))\n",
    "    print(\"Shape of Dataframe2: \"+str(df2.shape))\n",
    "     \n",
    "     \n",
    "    #Compare 2 dtaframes using Data Compy\n",
    "    compare = datacompy.Compare(\n",
    "                df1,\n",
    "                df2,\n",
    "                join_columns='Clinical Trial Identifier',  #You can also specify a list of columns\n",
    "                #abs_tol=0, #Optional, defaults to 0\n",
    "                #rel_tol=0, #Optional, defaults to 0\n",
    "                #df1_name='Original', #Optional, defaults to 'df1'\n",
    "                #df2_name='New' #Optional, defaults to 'df2'\n",
    "                )\n",
    "    ### compare.matches(ignore_extra_columns=False) = False ------> There is a change\n",
    "    ### compare.matches(ignore_extra_columns=False) = True -------> There is no change\n",
    "    ### Writing a logic to run when there is a change observed between two dataframes. ###\n",
    "    if compare.matches(ignore_extra_columns=False) == True:    ##### To check changes in 2 dataframes ######\n",
    "        print(\"There are no changes observed\")\n",
    "    \n",
    "    else:\n",
    "        print(compare.report())\n",
    "        data = compare.intersect_rows ### compare.intersect_rows will give a dataframe with combination of 3 dataframes, they are df1, df2 and match, \n",
    "        data.to_csv('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/comapre.intersect_rows_tt.csv') ### Store it in our location \n",
    "        #### Split data into 3 parts ##\n",
    "                \n",
    "        ####################### Part-1 - Dataframe1 #############################\n",
    "        data1 = data[['clinical trial identifier', 'competitor asset_df1', 'sponsor(s)_df1', 'trial title_df1', 'condition/disease_df1', \n",
    "                'trial phase_df1', 'trial status_df1', 'other trial identifier(s)_df1', 'target accrual_df1', 'actual accrual_df1', \n",
    "                'title acronym_df1', 'countries_df1', 'trial objective(s)_df1', 'study design_df1', 'patient segments_df1', 'treatment plan_df1', \n",
    "                'dosing/comparator_df1', 'roa_df1', 'outcome measures_df1', 'study start date_df1', 'primary completion date_df1', \n",
    "                'study completion date_df1', 'inclusion criteria_df1', 'exclusion criteria_df1', 'trial tag/attribute_df1', \n",
    "                'other trial identifier(s)_df1', 'trial outcome(s)_df1']]\n",
    "                \n",
    "        ### Renaming Column names ###\n",
    "        data1.columns = ['clinical trial identifier', 'competitor asset', 'sponsor(s)', 'trial title', 'condition/disease', 'trial phase',\n",
    "                        'trial status', 'other trial identifier(s)', 'target accrual', 'actual accrual', 'title acronym', 'countries', 'trial objective(s)', \n",
    "                        'study design', 'patient segments', 'treatment plan', 'dosing/comparator', 'roa', 'outcome measures', 'study start date', \n",
    "                        'primary completion date', 'study completion date', 'inclusion criteria', 'exclusion criteria', 'trial tag/attribute', \n",
    "                        'other trial identifier(s)', 'trial outcome(s)']\n",
    "                        \n",
    "        ####################### Part-2 -  Dataframe2 ##############################\n",
    "        data2 = data[['clinical trial identifier', 'competitor asset_df2', 'sponsor(s)_df2', 'trial title_df2', 'condition/disease_df2', 'trial phase_df2', \n",
    "                'trial status_df2', 'other trial identifier(s)_df2', 'target accrual_df2', 'actual accrual_df2', 'title acronym_df2', 'countries_df2', \n",
    "                'trial objective(s)_df2', 'study design_df2', 'patient segments_df2', 'treatment plan_df2', 'dosing/comparator_df2', 'roa_df2', 'outcome measures_df2', \n",
    "                'study start date_df2', 'primary completion date_df2', 'study completion date_df2', 'inclusion criteria_df2', 'exclusion criteria_df2',\n",
    "                'trial tag/attribute_df2', 'other trial identifier(s)_df2', 'trial outcome(s)_df2']]\n",
    "                \n",
    "        ### Renaming Column names ###\n",
    "        data2.columns = ['clinical trial identifier', 'competitor asset', 'sponsor(s)', 'trial title', 'condition/disease', 'trial phase',\n",
    "                        'trial status', 'other trial identifier(s)', 'target accrual', 'actual accrual', 'title acronym', 'countries', 'trial objective(s)', \n",
    "                        'study design', 'patient segments', 'treatment plan', 'dosing/comparator', 'roa', 'outcome measures', 'study start date', \n",
    "                        'primary completion date', 'study completion date', 'inclusion criteria', 'exclusion criteria', 'trial tag/attribute', \n",
    "                        'other trial identifier(s)','trial outcome(s)']\n",
    "        \n",
    "        ######################### Part-3 - Match-Dataframe ###########################\n",
    "        data_match = data[['clinical trial identifier', 'competitor asset_match', 'sponsor(s)_match', 'trial title_match', 'condition/disease_match', \n",
    "                    'trial phase_match', 'trial status_match', 'other trial identifier(s)_match', 'target accrual_match', 'actual accrual_match', \n",
    "                    'title acronym_match', 'countries_match', 'trial objective(s)_match', 'study design_match', 'patient segments_match', \n",
    "                    'treatment plan_match', 'dosing/comparator_match', 'roa_match', 'outcome measures_match', 'study start date_match',\n",
    "                    'primary completion date_match', 'study completion date_match', 'inclusion criteria_match', 'exclusion criteria_match',\n",
    "                    'trial tag/attribute_match', 'other trial identifier(s)_match', 'trial outcome(s)_match']]\n",
    "                    \n",
    "        ########## Finding row and column indications for Changes sheet ########\n",
    "        k=data_match.shape[0]     ## No.of Rows\n",
    "        l=data_match.shape[1]     ## No.of Columns\n",
    "        r=[]                      ## list to store row no's\n",
    "        c=[]                      ## list to store column no's\n",
    "        column=[]                 ## list to store columns names\n",
    "        NCT=[]                    ## list to store NCT no's\n",
    "        cmptrast=[]               ## Competitor Asset\n",
    "        condtn=[]                 ## Condition/Disease\n",
    "        acrnm=[]                  ## Title Acronym\n",
    "        #title=[]                  ## Trial Title  \n",
    "        cntris=[]                 ## Countries     \n",
    "        spnsr=[]                  ## Sponsor/Collaborator\n",
    "        trlphs=[]                 ## Trial Phase\n",
    "        status=[]                 ## Trial Status\n",
    "        d1=[]                     ## list to store df1 elements\n",
    "        d2=[]                     ## list to store df2 elements to know what is actual change\n",
    "        for j in range(1,l,1):\n",
    "            for i in range(0,k,1):\n",
    "                if (data_match.iloc[i][j]==False):\n",
    "                    print(\"column name:\",data1.iloc[:,j].name)\n",
    "                    column.append(data1.iloc[:,j].name)\n",
    "                    r.append(i)\n",
    "                    c.append(j)\n",
    "                    NCT.append(data1.iloc[i,0])\n",
    "                    cmptrast.append(data1.iloc[i,1])\n",
    "                    condtn.append(data1.iloc[i, 4])\n",
    "                    acrnm.append(data1.iloc[i, 10])\n",
    "                    cntris.append(data1.iloc[i, 11])\n",
    "                    spnsr.append(data1.iloc[i, 2])\n",
    "                    trlphs.append(data1.iloc[i, 5])\n",
    "                    status.append(data1.iloc[i, 6])            \n",
    "                    d1.append(data1.iloc[i,j])\n",
    "                    d2.append(data2.iloc[i,j])\n",
    "                    print(\"row index is: \",i)            \n",
    "                    print(\"column index is: \", j)\n",
    "                    print(\"--------------------------\")\n",
    "                    \n",
    "        ####################### Creating a dataframe to store changes ##############################\n",
    "        Changes = pd.DataFrame()\n",
    "        ## Storing lists as a column in a dataframe\n",
    "        Changes['Clinical Trial Identifier'] = NCT\n",
    "        Changes['Competitor Asset'] = cmptrast\n",
    "        Changes['Condition/Disease'] = condtn\n",
    "        Changes['Title Acronym'] = acrnm\n",
    "        Changes['Countries'] = cntris\n",
    "        Changes['Sponsor/Collaborator'] = spnsr\n",
    "        Changes['Trial Phase'] = trlphs\n",
    "        Changes['Trial Status'] = status\n",
    "        Changes['Column_Changes'] = column\n",
    "        Changes['Previous Value'] = d1\n",
    "        Changes['Current Value'] = d2\n",
    "        Changes['Date of update'] = date.today().strftime(\"%B %d, %Y\")\n",
    "        \n",
    "        #### Shape of Changes ####\n",
    "        print(\"Shape of Changes: \"+str(Changes.shape))\n",
    "        \n",
    "        ##################### Import Column Tags/Measures #######################\n",
    "        tag = pd.read_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/Trial Tag.xlsx\")\n",
    "        tag.head()\n",
    "        \n",
    "        ########### Merge tags column to Changes sheet #######################\n",
    "        Changes = Changes.merge(tag, on='Column_Changes')\n",
    "        Changes.columns\n",
    "       \n",
    "        ##### Sorting Changes ######\n",
    "        Changes = Changes.sort_values(['Clinical Trial Identifier', 'Trial Tag'], ascending=[True, True])\n",
    "        \n",
    "        today_changes = Changes[['Clinical Trial Identifier', 'Competitor Asset', 'Condition/Disease', 'Title Acronym',  'Countries', \n",
    "                                 'Sponsor/Collaborator', 'Trial Phase', 'Trial Status', 'Column_Changes', 'Trial Tag', 'Previous Value',\n",
    "                                 'Current Value', 'Date of update']]\n",
    "        \n",
    "        ##### Shape of Changes after adding Tag ####\n",
    "        print(\"Shape of changes after adding tag: \"+str(today_changes))\n",
    "        \n",
    "        \n",
    "        #### Export Today changes to current directory ####\n",
    "        today_changes.to_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/Changes\"+str(date.today())+\".xlsx\", index=False)\n",
    "        \n",
    "        ## Calling Existing Changes history and storing in a dataframe ####\n",
    "        changes_history = pd.read_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/Changes_History.xlsx\")\n",
    "        print(\"Shape of changes history: \",str(changes_history.shape))\n",
    "        #changes_history.columns\n",
    "        \n",
    "        \n",
    "        ########################### Appending Today Changes to existing changes history ###########################\n",
    "        frames = [today_changes, changes_history]  ### Createing a frame to add changes \n",
    "        history = pd.concat(frames)  ### Concatenate all dataframes in frames\n",
    "        print(\"history df shape: \", history.shape)\n",
    "        \n",
    "        ############### Exporting updated changes history with \"Changes_History.xlsx\" name to our directory for future use #####\n",
    "        history.to_excel(\"C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/Changes_History.xlsx\", index=False)\n",
    "\n",
    "        ##### Replacing todays data with base sheet for tomorrow comparison. Run this line of code after completing regular comparision task ####\n",
    "        data_comp.to_excel('C:/Users/11201312/Documents/Techwave/GSK_Pilot_Project03032020/CT_RA_Tracker1/ct_tt_ra.xlsx', index=False)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    RA()\n",
    "    \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "##########################################################################################\n",
    "        ### Appending RA, PSA, SPA CT-TT data details to display in CI Dashboard ###\n",
    "##########################################################################################\n",
    "\n",
    "CA_Display = pd.read_excel(\"C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CT_RA_Tracker1\\\\ct_tt_ra_display.xlsx\")\n",
    "print(\"CA-Display as on date: \", CA_Display.shape)\n",
    "\n",
    "################## Concatenate frames and removing duplicates #######################\n",
    "frames = [CA_Display]  ### Createing a frame to append CT & TT trials \n",
    "RA_PSA_SPA_Display = pd.concat(frames).drop_duplicates().reset_index(drop=True)  ### Concatenate all dataframes in frames\n",
    "print(\"CT-TT RA_PSA_SPA_Display as on date: \", RA_PSA_SPA_Display.shape)\n",
    "\n",
    "### Replacing empty cells with \"Null\"\n",
    "RA_PSA_SPA_Display1=RA_PSA_SPA_Display.fillna(\"Null\")\n",
    "\n",
    "### Export Changes Data to Excel ###\n",
    "RA_PSA_SPA_Display1.to_excel('C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CI_Dashboard\\\\TrialDataDetails.xlsx', index=False)\n",
    "\n",
    "##############################################################################################\n",
    "        ### Appending RA, PSA, SPA_Changes_History data to display in CI Dashboard ###\n",
    "##############################################################################################\n",
    "CA_Changes = pd.read_excel(\"C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CT_RA_Tracker1\\\\Changes_History.xlsx\")\n",
    "print(\"CA-Changes as on date: \", CA_Changes.shape)\n",
    "\n",
    "################## Concatenate frames and removing duplicates #######################\n",
    "frames = [CA_Changes]  ### Createing a frame to append CT & TT trials \n",
    "RA_PSA_SPA_Changes = pd.concat(frames).drop_duplicates().reset_index(drop=True)  ### Concatenate all dataframes in frames\n",
    "print(\"RA_PSA_SPA_Changes as on date: \", RA_PSA_SPA_Changes.shape)\n",
    "\n",
    "RA_PSA_SPA_Changes = RA_PSA_SPA_Changes.sort_values(['Clinical Trial Identifier', 'Trial Tag', 'Date of update'], \n",
    "                                                     ascending=[True, True, True])\n",
    "### Replacing empty cells with \"Null\"\n",
    "RA_PSA_SPA_Changes1=RA_PSA_SPA_Changes.fillna(\"Null\")\n",
    "\n",
    "### Export Changes Data to Excel ###\n",
    "RA_PSA_SPA_Changes1.to_excel('C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CI_Dashboard\\\\Changes_History.xlsx', index=False)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "### Appending RA, PSA, SPA latest updated data to dosplay in CI Dashboard ###\n",
    "##########################################################################################\n",
    "\n",
    "CA_Updated = pd.read_excel(\"C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CT_RA_Tracker1\\\\ct_tt_ra_Updated\"+str(date.today())+'.xlsx', index=False)\n",
    "\n",
    "print(\"CA-Updated as on date: \", CA_Updated.shape)\n",
    "\n",
    "################## Concatenate frames and removing duplicates #######################\n",
    "frames = [CA_Updated]  ### Createing a frame to append CT & TT trials \n",
    "RA_PSA_SPA_Updated = pd.concat(frames).drop_duplicates().reset_index(drop=True)  ### Concatenate all dataframes in frames\n",
    "print(\"RA_PSA_SPA_Updated as on Date: \", RA_PSA_SPA_Updated.shape)\n",
    "\n",
    "### Replacing empty cells with \"Null\"\n",
    "RA_PSA_SPA_Updated1=RA_PSA_SPA_Updated.fillna(\"Null\")\n",
    "\n",
    "### Export updated Data to Excel ###\n",
    "RA_PSA_SPA_Updated1.to_excel('C:\\\\Users\\\\11201312\\\\Documents\\\\Techwave\\\\GSK_Pilot_Project03032020\\\\CI_Dashboard\\\\TrialLatestData.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
